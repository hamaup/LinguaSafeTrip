# backend/app/agents/safety_beacon_agent/main_orchestrator.py
import logging
import os
import uuid
import json
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from functools import lru_cache
from app.schemas.agent import AgentResponse
from fastapi import HTTPException
from pydantic import BaseModel, ValidationError
import asyncio

from langgraph.graph import END # LangGraphã®çµ‚äº†çŠ¶æ…‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# LangChain Core & Google & Firestore
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, trim_messages
from langchain_google_firestore import FirestoreChatMessageHistory # history_managerçµŒç”±ã§åˆ©ç”¨

# ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from ..managers.disaster_context_manager import update_context # update_contextã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..managers.user_state_manager import get_user_disaster_state, update_user_disaster_state
from app.prompts.disaster_prompts import get_disaster_prompt, get_proactive_prompt
# Translation tool is imported inside functions to avoid circular import
# from .emergency_integration import check_and_handle_emergency  # å‰Šé™¤ï¼šç·Šæ€¥æ¤œçŸ¥çµ±åˆ
from app.config import app_settings
from app.config.timeout_settings import TimeoutSettings
from .llm_singleton import get_llm_client # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
from app.prompts.prompts import SYSTEM_PROMPT_TEXT # ãƒ¡ã‚¤ãƒ³ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
from ..tool_definitions import tools # ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
from ..managers.history_manager import get_chat_message_history # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç®¡ç†
from ..managers.integrated_memory_manager import IntegratedMemoryManager # çµ±åˆãƒ¡ãƒ¢ãƒªç®¡ç†
from ..callbacks import SmsToolResultCallbackHandler # SMSãƒ„ãƒ¼ãƒ«ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
from .graph_builder import create_unified_graph as create_safety_beacon_agent_graph # çµ±åˆgraphä½¿ç”¨
# Import removed - analysis now done in LangGraph
# from ..handlers.off_topic_handler import ImprovedOffTopicHandler

# ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
logger = logging.getLogger(__name__)

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚­ãƒ¼ãƒ
from app.schemas.agent import AgentStateModel
from app.schemas.agent.suggestions import SuggestionItem, SuggestionCard
# Note: AgentUserLocation, AgentUserProfile removed as they were not found in codebase
from app.schemas.agent.suggestions import ProactiveSuggestionContext
from app.schemas.chat_schemas import ChatRequest

# --- LangGraphã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ï¼ˆLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–ï¼‰ ---
@lru_cache(maxsize=1)
def get_compiled_graph():
    """ã‚°ãƒ©ãƒ•ã‚’ä¸€åº¦ã ã‘ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ï¼‰"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
        llm = get_llm_client(task_type="response_generation")
        compiled_graph = create_safety_beacon_agent_graph(llm)
        logger.info("SafetyBeacon LangGraph compiled and cached with LRU")
        return compiled_graph
    except Exception as e_graph_compile:
        logger.error(f"Failed to compile SafetyBeacon LangGraph: {e_graph_compile}", exc_info=True)
        raise RuntimeError(f"Graph compilation failed: {e_graph_compile}")

def clear_graph_cache():
    """ã‚°ãƒ©ãƒ•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    get_compiled_graph.cache_clear()
    logger.info("Graph cache cleared")

async def run_agent_interaction(request: ChatRequest) -> AgentResponse:
    """
    çµ±åˆãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ã—ãŸLangGraphãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
    """
    start_time_utc = datetime.now(timezone.utc)
    device_identifier = request.device_id

    # Starting agent interaction with integrated memory
    logger.info(f"Running agent interaction for device: {device_identifier}")

    if not app_settings.gcp_project_id:
         logger.error("GCP_PROJECT_ID is not set. Cannot proceed.")
         return AgentResponse(
             response_text="ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå¿œç­”ã§ãã¾ã›ã‚“ã€‚",
             session_id=request.session_id or "error",
             current_task_type="error",
             is_emergency_response=False
         )

    compiled_agent_graph = get_compiled_graph()
    if not compiled_agent_graph:
        logger.error("Agent graph is not compiled. Cannot process request.")
        return AgentResponse(
             response_text="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
             session_id=request.session_id or "error", 
             current_task_type="error",
             is_emergency_response=False
         )

    # --- çµ±åˆãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ– ---
    memory_manager = IntegratedMemoryManager(compiled_agent_graph)
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰IDç”Ÿæˆã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³IDæ±ºå®š
    thread_id = memory_manager.generate_thread_id(request.session_id, device_identifier)
    session_id = memory_manager.extract_session_id(thread_id)
    
    # --- çµ±åˆå±¥æ­´å–å¾— ---
    # åˆæœŸæ®µéšã®ä¸¦åˆ—å‡¦ç†å®Ÿè£…ï¼ˆ2-3å€é«˜é€ŸåŒ–ï¼‰
    async def get_integrated_history():
        try:
            history = await memory_manager.sync_histories(
                thread_id, session_id, device_identifier
            )
            return history
        except Exception as e_hist:
            logger.error(f"Failed to get integrated history: {e_hist}", exc_info=True)
            return []
    
    async def parse_user_location():
        if not request.user_location:
            return None
            
        loc_data = request.user_location
        try:
            # AgentUserLocation class not found - using dict instead
            return {
                'latitude': loc_data.get('latitude'),
                'longitude': loc_data.get('longitude'),
                'accuracy': loc_data.get('accuracy'),
                'source': loc_data.get('source'),
                'last_updated': loc_data.get('timestamp') or start_time_utc.isoformat()
            }
        except Exception as e_loc_parse:
            logger.error(f"Could not parse user location data: {loc_data}. Error: {e_loc_parse}", exc_info=True)
            return None
    
    async def get_device_data():
        try:
            from app.services.device_service import get_device_by_id
            return await get_device_by_id(device_identifier)
        except Exception as e:
            logger.warning(f"ãƒ‡ãƒã‚¤ã‚¹çŠ¶æ³å–å¾—å¤±æ•—: {e}")
            return None
    
    # ä¸¦åˆ—å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
    
    async def with_timeout(coro, timeout_seconds, default=None):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ"""
        try:
            return await asyncio.wait_for(coro, timeout=timeout_seconds)
        except asyncio.TimeoutError:
            logger.warning(f"Task timed out after {timeout_seconds}s")
            return default
    
    # å„ã‚¿ã‚¹ã‚¯ã«ç¾å®Ÿçš„ãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
    parallel_tasks = await asyncio.gather(
        with_timeout(get_integrated_history(), TimeoutSettings.HISTORY_FETCH, []),  # å±¥æ­´ã¯5ç§’
        with_timeout(parse_user_location(), TimeoutSettings.LOCATION_PARSE, None),  # ä½ç½®æƒ…å ±ã¯3ç§’
        with_timeout(get_device_data(), TimeoutSettings.DEVICE_DATA_FETCH, None),  # ãƒ‡ãƒã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯4ç§’
        return_exceptions=True
    )
    
    integrated_history = parallel_tasks[0] if not isinstance(parallel_tasks[0], Exception) else []
    user_location_pydantic = parallel_tasks[1] if not isinstance(parallel_tasks[1], Exception) else None
    device_data_from_parallel = parallel_tasks[2] if not isinstance(parallel_tasks[2], Exception) else None
    
    if user_location_pydantic:
        pass
    
    # user_profile_pydantic removed - AgentUserProfile class not found in codebase
    user_profile_pydantic = None

    # å…¥åŠ›æ¤œè¨¼
    if not request.user_input or not isinstance(request.user_input, str):
        logger.error(f"Invalid user input: {request.user_input}")
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨€èªã«å¯¾å¿œ
        error_msg = "Invalid input."
        if request.user_language:
            from app.tools.translation_tool import TranslationTool
            translator = TranslationTool()
            try:
                error_msg = await translator.translate(
                    text=error_msg,
                    target_language=request.user_language,
                    source_language="en"
                )
            except:
                pass  # ç¿»è¨³å¤±æ•—æ™‚ã¯è‹±èªã®ã¾ã¾
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚AgentResponseã‚’è¿”ã™
        from app.schemas.common.enums import TaskType
        return AgentResponse(
            response_text=error_msg,
            current_task_type=TaskType.ERROR,
            status="error",
            is_emergency_response=False,
            session_id=session_id
        )

    # è¨€èªã‚³ãƒ¼ãƒ‰ã‚’æ­£è¦åŒ–ï¼ˆzh-CN -> zh_CN, zh-TW -> zh_TW ãªã©ï¼‰
    def normalize_language_code(lang_code: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’æ­£è¦åŒ– - ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›"""
        # ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
        normalized = lang_code.replace('-', '_')
        
        # çŸ­ç¸®å½¢ã®å‡¦ç†
        if normalized == 'zh':
            return 'zh_CN'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç°¡ä½“ä¸­æ–‡
        
        # æ—¢çŸ¥ã®è¨€èªã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        language_mapping = {
            'zh_CN': 'zh_CN',  # ç°¡ä½“ä¸­æ–‡
            'zh_TW': 'zh_TW',  # ç¹ä½“ä¸­æ–‡
            'pt_BR': 'pt',     # ãƒãƒ«ãƒˆã‚¬ãƒ«èªï¼ˆãƒ–ãƒ©ã‚¸ãƒ«ï¼‰-> pt
            'ko_KR': 'ko',     # éŸ“å›½èª
            'ja_JP': 'ja',     # æ—¥æœ¬èª
            'en_US': 'en',     # è‹±èª
            'en_GB': 'en',     # è‹±èª
            'es_ES': 'es',     # ã‚¹ãƒšã‚¤ãƒ³èª
            'fr_FR': 'fr',     # ãƒ•ãƒ©ãƒ³ã‚¹èª
            'de_DE': 'de',     # ãƒ‰ã‚¤ãƒ„èª
            'it_IT': 'it',     # ã‚¤ã‚¿ãƒªã‚¢èª
            'ru_RU': 'ru',     # ãƒ­ã‚·ã‚¢èª
        }
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯å¤‰æ›ã€ãªã‘ã‚Œã°ãã®ã¾ã¾è¿”ã™
        return language_mapping.get(normalized, normalized)
    
    # Language settings
    
    normalized_user_language = normalize_language_code(request.user_language)
    detected_language = normalize_language_code(getattr(request, 'detected_language', request.user_language))
    
    # --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ ---
    # response_cache module was deleted, so we skip template checking

    
    # Analysis will be performed within LangGraph to avoid duplication
    user_input_for_processing = request.user_input
    
    # Set defaults - actual analysis will happen in LangGraph nodes
    is_disaster_mode_computed = request.is_disaster_mode
    
    # ä¸¦åˆ—å‡¦ç†ã§å–å¾—ã—ãŸãƒ‡ãƒã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    if device_data_from_parallel and hasattr(device_data_from_parallel, 'current_mode'):
        device_current_mode = device_data_from_parallel.current_mode
        # ãƒ‡ãƒã‚¤ã‚¹ãŒç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å„ªå…ˆ
        if device_current_mode == "emergency":
            is_disaster_mode_computed = True
        # Device mode: {device_current_mode}
    else:
        # No device data or mode information
        pass
    
    mapped_task_type = "unknown"  # Will be determined by LangGraph
    
    # Request parameters processed

    # åˆ†æçµæœã‚’ä½¿ã£ã¦åˆæœŸçŠ¶æ…‹ã‚’è¨­å®š - å¤šè¨€èªã®ã¾ã¾å‡¦ç†
    initial_agent_state = AgentStateModel(
        device_id=device_identifier,
        session_id=session_id,
        user_input=user_input_for_processing,  # å¤šè¨€èªã®ã¾ã¾å‡¦ç†
        current_user_input=user_input_for_processing,  # å¤šè¨€èªã®ã¾ã¾å‡¦ç†
        messages=[HumanMessage(content=user_input_for_processing)],  # å¤šè¨€èªã§å‡¦ç†
        chat_history=integrated_history,
        user_language=normalized_user_language,
        detected_language=detected_language,
        is_disaster_mode=is_disaster_mode_computed,  # è¨ˆç®—æ¸ˆã¿ã®ç½å®³ãƒ¢ãƒ¼ãƒ‰
        user_location=request.user_location,  # ç›´æ¥è¾æ›¸å½¢å¼ã§æ¸¡ã™
        user_profile=user_profile_pydantic,
        local_contact_count=request.local_contact_count if request.local_contact_count is not None else 0,
        current_datetime_utc=start_time_utc.isoformat(),
        current_task_type=mapped_task_type,  # Will be determined by LangGraph
        primary_intent="unknown",  # Will be determined by LangGraph
        external_alerts=request.external_alerts or [],  # ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆè¿½åŠ 
        extracted_entities={"search_keywords": []},  # Will be populated by LangGraph
        disaster_relevance=0.0,  # Will be determined by LangGraph
        emotional_tone="neutral",
        required_action="none",  # Will be determined by LangGraph
        secondary_intents=[],
        final_response_text=None,
        cards_to_display_queue=[],
        requires_action_data=None,
        generated_cards_for_frontend=[],
        intermediate_results={},  # Will be populated by LangGraph
        disaster_context_evaluation={
            "needs_location": False  # Will be determined by LangGraph
        },
        chat_records=[],
        language_confidence=1.0 if request.user_language else 0.0
    )
    # åˆæœŸçŠ¶æ…‹ãƒ­ã‚°ã¯å‰Šé™¤

    # --- 2. å±¥æ­´ãƒˆãƒªãƒŸãƒ³ã‚° (messagesãƒãƒ£ãƒãƒ«ã¯LangGraphãŒè‡ªå‹•ã§ç®¡ç†ã™ã‚‹ãŸã‚ã€chat_history_lcã‚’ãƒˆãƒªãƒŸãƒ³ã‚°) ---
    system_prompt_approx_tokens = count_tokens_approximated(SYSTEM_PROMPT_TEXT)
    user_input_tokens = count_tokens_approximated(initial_agent_state.user_input)
    effective_max_history_tokens = app_settings.tokens.max_history_tokens - system_prompt_approx_tokens - user_input_tokens

    if effective_max_history_tokens < 0 : effective_max_history_tokens = 0
    # History token trimming: effective_max={effective_max_history_tokens}

    trimmed_history = trim_messages(
        messages=initial_agent_state.chat_history,
        max_tokens=effective_max_history_tokens,
        strategy="last",
        token_counter=lambda x: count_tokens_approximated(str(x.content if hasattr(x, 'content') else x)),
    )
    initial_agent_state.chat_history = trimmed_history
    # History trimmed: {len(integrated_history)} -> {len(trimmed_history)} messages

    # --- 3. æœ€é©åŒ–: äº‹å‰åˆ†ææ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ— ---
    # ä¸¦åˆ—åˆ†æã§æ—¢ã«ç½å®³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨æ„å›³ã¯åˆ†ææ¸ˆã¿
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¿œç­”ã®ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€ŸåŒ–ï¼‰


    # --- 3.5. ç·Šæ€¥æ¤œçŸ¥ ---
    # emergency_info = await check_and_handle_emergency({
    #     "user_input": request.user_input,  # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®å…¥åŠ›ã‚’ä½¿ç”¨
    #     "user_location": request.user_location,
    #     "external_alerts": request.external_alerts or [],
    #     "recent_alerts": []
    # })
    emergency_info = {"is_emergency": False, "emergency_level": 0, "emergency_actions": None}
    
    if emergency_info["is_emergency"]:
        logger.warning(f"ğŸš¨ Emergency detected: level={emergency_info['emergency_level']}, actions={len(emergency_info.get('emergency_actions', []))}")
    
    # --- 4. LangGraph ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ ---
    final_state: Optional[AgentStateModel] = None
    try:
        # çŠ¶æ…‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‹ãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not isinstance(initial_agent_state, (dict, AgentStateModel)):
            logger.error(
                f"Invalid initial state type: {type(initial_agent_state)}. "
                f"Content: {str(initial_agent_state)[:200]}... "
                f"Converting to dict"
            )
            initial_agent_state = initial_agent_state.dict() if hasattr(initial_agent_state, 'dict') else {}

        graph_config = {"configurable": {"thread_id": thread_id}}

        # Invoking agent graph for session: {session_id}
        
        # åˆ†æçµæœã‹ã‚‰æ—¢ã«å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’ä½¿ç”¨
        input_state = {
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            "conversation_id": request.session_id,
            "device_id": device_identifier,  # ãƒ‡ãƒã‚¤ã‚¹IDã‚’è¿½åŠ 
            "session_id": session_id,  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚‚è¿½åŠ 
            "user_input": user_input_for_processing,  # å¤šè¨€èªã®ã¾ã¾ä½¿ç”¨
            "current_user_input": user_input_for_processing,
            "chat_history": integrated_history,
            "messages": [],  # messagesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åˆæœŸåŒ–
            "is_disaster_mode": is_disaster_mode_computed,  # è¨ˆç®—æ¸ˆã¿ã®å€¤ã‚’ä½¿ç”¨
            "user_location": request.user_location,  # ä½ç½®æƒ…å ±ã‚’è¿½åŠ 
            "secondary_intents": [],
            "chat_records": [],
            "current_task_type": mapped_task_type,
            "generated_cards_for_frontend": [],
            "cards_to_display_queue": [],
            "recent_alerts": request.external_alerts or [],  
            "external_alerts": request.external_alerts or [],  
            "suggested_actions": [],  # Will be determined by LangGraph
            "primary_intent": "unknown",  # Will be determined by LangGraph
            "turn_count": 0,
            "is_disaster_related": False,  # Will be determined by LangGraph
            "intent_confidence": 0.5,  # Default to 0.5 to avoid low confidence routing
            # è¨€èªæ¤œå‡ºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åˆæœŸåŒ–
            "user_language": normalized_user_language,  
            "detected_language": detected_language,  
            "language_confidence": 1.0 if request.user_language else 0.0,  
            # ãã®ä»–ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            "parallel_updates": [],
            "required_action": "none",  # Will be determined by LangGraph  
            "emotional_tone": "neutral",
            "extracted_entities": {"search_keywords": []},  # Will be populated by LangGraph
            "disaster_relevance": 0.0,  # Will be determined by LangGraph
            # Optional fields from AgentState
            "current_disaster_info": None,
            "error_message": None,
            "requires_professional_handling": False,
            # Missing fields
            "routing_decision": None,
            "last_askuser_reason": None,
            "intermediate_results": {},  # Will be populated by LangGraph
            "final_response_text": None,
            "off_topic_response": None,
            # Emergency detection fields
            "is_emergency_response": emergency_info["is_emergency"],
            "emergency_level": emergency_info["emergency_level"],
            "emergency_actions": emergency_info["emergency_actions"],
            "emergency_message": emergency_info.get("emergency_message"),
            # User app status - CRITICAL: This was missing!
            "local_contact_count": request.local_contact_count if request.local_contact_count is not None else 0
        }

        try:
            # Starting LangGraph execution
            
            # Execute graph with timeout
            events = []
            start_time = asyncio.get_event_loop().time()
            
            # Create trace file
            trace_file = f"/tmp/langgraph_trace_{session_id}.log"
            with open(trace_file, "w") as f:
                f.write(f"LangGraph Execution Trace - Session: {session_id}\n")
                f.write(f"Start time: {datetime.now(timezone.utc)}\n")
                f.write(f"="*60 + "\n")
            
            try:
                # Try invoke instead of astream to see if it completes
                # Use invoke with timeout
                # çµ±åˆãƒ¡ãƒ¢ãƒªå¯¾å¿œã®configè¨­å®š
                config = {
                    "configurable": {
                        "thread_id": thread_id,
                        "session_id": session_id,
                        "device_id": device_identifier
                    },
                    "recursion_limit": app_settings.graph.recursion_limit,
                    "max_retries": app_settings.graph.max_retries,
                }
                
                final_state = await asyncio.wait_for(
                    compiled_agent_graph.ainvoke(
                        input_state,
                        config=config
                    ),
                    timeout=app_settings.graph.timeout
                )
                
                elapsed = asyncio.get_event_loop().time() - start_time
                # Graph execution completed
                
                with open(trace_file, "a") as f:
                    f.write(f"\nExecution completed in {elapsed:.1f}s\n")
                    f.write(f"Final state keys: {list(final_state.keys()) if isinstance(final_state, dict) else 'Not a dict'}\n")
                    
            except asyncio.TimeoutError:
                elapsed = asyncio.get_event_loop().time() - start_time
                logger.error(f"Graph execution timeout after {elapsed:.1f} seconds")
                with open(trace_file, "a") as f:
                    f.write(f"\nTIMEOUT after {elapsed:.1f}s\n")
                raise Exception("Graph execution timeout")
            
            # LangGraph execution completed
        except Exception as e:
            logger.error(f"Error during agent graph execution: {e}", exc_info=True)
            logger.error(f"Exception type: {type(e).__name__}")
            if hasattr(e, '__cause__') and e.__cause__:
                logger.error(f"Caused by: {e.__cause__}")
            raise

        # ã‚°ãƒ©ãƒ•ã‹ã‚‰æœ€çµ‚çŠ¶æ…‹ã‚’å–å¾—
        # LangGraphã®çŠ¶æ…‹ã‚’å–å¾—
        graph_final_state_snapshot = compiled_agent_graph.get_state(config=graph_config)
        if graph_final_state_snapshot and graph_final_state_snapshot.values:
            final_state_raw = graph_final_state_snapshot.values
            
            with open("/tmp/orchestrator_debug.log", "w") as f:
                f.write(f"Final state keys: {list(final_state_raw.keys())}\n")
                f.write(f"final_response_text: {final_state_raw.get('final_response_text', 'NOT FOUND')}\n")
                f.write(f"off_topic_response: {final_state_raw.get('off_topic_response', 'NOT FOUND')}\n")
                f.write(f"Full state (truncated):\n")
                for key, value in final_state_raw.items():
                    value_str = str(value)[:200] if value else "None"
                    f.write(f"  {key}: {value_str}\n")
            
            # LangGraphã®çŠ¶æ…‹ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡º
            try:
                # æœ€çµ‚å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                final_response_text = final_state_raw.get("final_response_text", "")
                off_topic_response = final_state_raw.get("off_topic_response", "")
                
                # messagesã‹ã‚‰å¿œç­”ã‚’å–å¾—ï¼ˆç½å®³æƒ…å ±ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã®å¿œç­”ï¼‰
                messages = final_state_raw.get("messages", [])
                message_response = ""
                if messages and len(messages) > 0:
                    last_message = messages[-1]
                    if hasattr(last_message, 'content'):
                        message_response = last_message.content
                
                # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æ±ºå®šï¼ˆfinal_response_textã‚’å„ªå…ˆï¼‰
                response_text = final_response_text or off_topic_response or message_response or "Unable to generate response."
                
                # å‰Šé™¤: ç¿»è¨³ã¯enhance_qualityãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œæ¸ˆã¿
                
                # ã‚«ãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—ï¼ˆcards_to_display_queueãŒä¸»è¦ãªã‚½ãƒ¼ã‚¹ï¼‰
                cards = final_state_raw.get("cards_to_display_queue", [])
                if not cards:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦generated_cards_for_frontendã‚’ç¢ºèª
                    cards = final_state_raw.get("generated_cards_for_frontend", [])
                
                # Card counts: display_queue={len(final_state_raw.get('cards_to_display_queue', []))}, generated={len(final_state_raw.get('generated_cards_for_frontend', []))}, final={len(cards)}
                
                # Card validation completed
                
                # ä¼šè©±æƒ…å ±ã‚’æ›´æ–°
                session_info = {
                    "turn_count": int(final_state_raw.get("turn_count", 0)),
                    "is_disaster_mode": final_state_raw.get("is_disaster_mode", False),
                    "primary_intent": str(final_state_raw.get("primary_intent", "unknown")).replace("IntentCategory.", "").lower()
                }
                
                # Response extracted successfully
                
                # ãƒ‡ãƒãƒƒã‚°: åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼ã‚’ç¢ºèª
                # åˆ©ç”¨å¯èƒ½ã‚­ãƒ¼ãƒ­ã‚°ã¯å‰Šé™¤
                
                # ç·Šæ€¥æ™‚å¿œç­”æƒ…å ±ã‚’å–å¾—
                is_emergency_response = final_state_raw.get("is_emergency_response", False)
                emergency_level = final_state_raw.get("emergency_level", None)
                emergency_actions = final_state_raw.get("emergency_actions", None)
                
                # SMSæ„å›³ã‚’æ¤œå‡ºã—ã¦ã‚«ãƒ¼ãƒ‰ã‚’å‹•çš„ç”Ÿæˆ
                primary_intent = final_state_raw.get("primary_intent", "unknown")
                extracted_entities = final_state_raw.get("extracted_entities", {})
                
                # CLAUDE.mdåŸå‰‡: æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã§ã¯ãªãLLMã®æ„å›³åˆ†é¡çµæœã‚’ä¿¡é ¼
                # intent_routerãŒæ—¢ã«æ­£ç¢ºã«åˆ†é¡ã—ã¦ã„ã‚‹ãŸã‚ã€è¿½åŠ ã®æ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯ã¯ä¸è¦
                is_sms_intent = (
                    primary_intent == "safety_confirmation" or
                    str(primary_intent) == "safety_confirmation"
                )
                
                # LLMãƒ™ãƒ¼ã‚¹ã®SMSæ„å›³æ¤œå‡ºï¼ˆCLAUDE.mdåŸå‰‡æº–æ‹ ï¼‰
                user_input = final_state_raw.get("user_input", "") or final_state_raw.get("current_user_input", "") or request.user_input
                has_sms_keywords = await _detect_sms_intent_with_llm(user_input)
                
                if is_sms_intent or has_sms_keywords:
                    logger.info(f"SMS intent detected: is_sms_intent={is_sms_intent}, has_sms_keywords={has_sms_keywords}")
                    # SMSç¢ºèªãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦ requires_action ã¨ action_data ã‚’å–å¾—
                    from ..handlers.sms_confirmation_handler import handle_sms_confirmation_request
                    
                    # SMSç¢ºèªãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç”¨ã®çŠ¶æ…‹ã‚’æº–å‚™
                    sms_state = final_state_raw.copy()
                    sms_state["primary_intent"] = "safety_confirmation"  # Force SMS intent
                    sms_state["user_input"] = request.user_input  # Use original request input
                    sms_state["user_location"] = request.user_location  # Ensure location is available
                    sms_state["is_disaster_mode"] = final_state_raw.get("is_disaster_mode", False)
                    sms_state["local_contact_count"] = final_state_raw.get("local_contact_count", request.local_contact_count or 0)
                    
                    try:
                        # Pass user language for multilingual SMS support
                        user_language = request.user_language or 'ja'
                        sms_result = await handle_sms_confirmation_request(sms_state, target_language=user_language)
                        
                        logger.info(f"SMS handler result: requires_action={sms_result.get('requires_action')}")
                        # SMSçµæœã‹ã‚‰ requires_action ã¨ action_data ã‚’æŠ½å‡º
                        if sms_result.get("requires_action"):
                            final_state_raw["requires_action"] = sms_result["requires_action"]
                            final_state_raw["action_data"] = sms_result["action_data"]
                            logger.info(f"Set requires_action: {final_state_raw['requires_action']}")
                            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚‚SMSãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã®ã‚‚ã®ã‚’ä½¿ç”¨
                            if sms_result.get("final_response_text"):
                                response_text = sms_result["final_response_text"]
                                final_state_raw["final_response_text"] = response_text
                                
                    except Exception as sms_error:
                        logger.error(f"Error calling SMS confirmation handler: {sms_error}", exc_info=True)
                else:
                    # SMSæ„å›³éæ¤œå‡ºãƒ­ã‚°ã¯å‰Šé™¤
                    pass
                
                # ç·Šæ€¥å¿œç­”ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡ºã«ã‚ˆã‚‹ä»£æ›¿æ‰‹æ®µ - LLMãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶è¨€èªåˆ†æ
                if not is_emergency_response and response_text:
                    emergency_content_detected = await _detect_emergency_content_semantic(response_text)
                    
                    # å¤–éƒ¨ã‚¢ãƒ©ãƒ¼ãƒˆã®å­˜åœ¨ç¢ºèª
                    has_alerts = (final_state_raw.get("recent_alerts") or 
                                final_state_raw.get("external_alerts") or 
                                request.external_alerts)
                    
                    if emergency_content_detected and has_alerts:
                        # Emergency response detected by content analysis
                        is_emergency_response = True
                        emergency_level = await _determine_emergency_level_semantic(response_text)
                        emergency_actions = ["ç›´ã¡ã«å®‰å…¨ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„", "é¿é›£æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„"]
                
                if is_emergency_response:
                    logger.info(f"Emergency detected: level={emergency_level}")
                
                # å¿…è¦ãªæƒ…å ±ã‚’å«ã‚€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                # ç·Šæ€¥ãƒ¬ãƒ™ãƒ«ã®çµ±åˆ
                final_emergency_level = emergency_level or final_state_raw.get("emergency_level")
                emergency_level_int = emergency_info["emergency_level"] if emergency_info["is_emergency"] else 0
                if final_emergency_level and not emergency_info["is_emergency"]:
                    emergency_level_int = _convert_emergency_level_to_int(final_emergency_level)
                
                final_state = {
                    "final_response_text": response_text,  # çµ±ä¸€ã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ä½¿ç”¨
                    "cards": cards,
                    "session_info": session_info,
                    "is_emergency_response": is_emergency_response or emergency_info["is_emergency"],
                    "emergency_level": final_emergency_level or emergency_info["emergency_level"],
                    "emergency_actions": emergency_actions or emergency_info["emergency_actions"],
                    "emergency_level_int": emergency_level_int
                }
            except Exception as e_extract:
                logger.error(f"Error extracting data from graph state: {e_extract}", exc_info=True)
                final_state = None
        else:
            logger.error("Failed to obtain final state snapshot or its values from graph execution.")
            final_state = None

        if not final_state:
            logger.error("Failed to obtain or validate final_state from graph execution stream.")
            from app.schemas.common.enums import TaskType
            return AgentResponse(
                response_text="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ(çŠ¶æ…‹å–å¾—å¤±æ•—)ã€‚",
                current_task_type=TaskType.ERROR,
                status="error",
                is_emergency_response=emergency_info["is_emergency"],
                emergency_actions=emergency_info["emergency_actions"],
                debug_info={
                    "error": "Failed to obtain final state",
                    "emergency_level_int": emergency_info["emergency_level"]
                },
                session_id=session_id
            )

    except Exception as e_graph_run:
        logger.error(f"Error during agent graph execution for {session_id}: {e_graph_run}", exc_info=True)
        error_message = {
            "role": "system",
            "content": f"Agent processing error: {str(e_graph_run)}",
            "name": "error_handler"
        }
        try:
            error_message = initial_agent_state.validate_messages([error_message])[0]
        except Exception:
            error_message = {
                "role": "system",
                "content": "An error occurred during agent processing",
                "name": "error_handler"
            }
        from app.schemas.common.enums import TaskType
        return AgentResponse(
            response_text=error_message["content"],
            current_task_type=TaskType.ERROR,
            status="error",
            is_emergency_response=emergency_info["is_emergency"],
            emergency_actions=emergency_info["emergency_actions"],
            debug_info={
                "error": str(e_graph_run),
                "error_type": type(e_graph_run).__name__,
                "emergency_level_int": emergency_info["emergency_level"]
            },
            session_id=session_id
        )

    # final_stateã¯è¾æ›¸å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿
    # æœ€çµ‚çŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ãƒ³ãƒ—ã¯å‰Šé™¤

    # --- 5. æœ€çµ‚å¿œç­”ã®çµ„ã¿ç«‹ã¦ ---
    # Handle both dict and AddableValuesDict (LangGraph output)
    if hasattr(final_state, 'get') or isinstance(final_state, dict):
        response_text_to_user = final_state.get("final_response_text", "")
        # Final response prepared: length={len(response_text_to_user)}
        # Use cards from the enhanced processing above (not directly from final_state)
        api_cards: List[Dict[str, Any]] = cards if 'cards' in locals() else final_state.get("cards_to_display_queue", [])
        # API cards: count={len(api_cards)}
    else:
        # final_stateãŒè¾æ›¸ã®ã‚ˆã†ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ãªã„å ´åˆ
        logger.warning(f"Unexpected final_state type: {type(final_state)}")
        response_text_to_user = ""
        api_cards = []
        
    if not response_text_to_user:
        response_text_to_user = "Processing completed but no message to display."
        # Using default response text
    
    # Ensure all cards are serialized to dictionaries to prevent JSON serialization errors
    serialized_api_cards = []
    for card in api_cards:
        try:
            if hasattr(card, 'model_dump'):
                # Pydantic v2 model
                serialized_api_cards.append(card.model_dump())
            elif hasattr(card, 'dict'):
                # Pydantic v1 model
                serialized_api_cards.append(card.dict())
            elif isinstance(card, dict):
                # Already a dictionary
                serialized_api_cards.append(card)
            else:
                # Try to convert to dict manually
                logger.warning(f"Unknown card type: {type(card)}, attempting manual conversion")
                card_dict = {
                    "card_id": getattr(card, 'card_id', str(id(card))),
                    "card_type": getattr(card, 'card_type', 'unknown'),
                    "title": getattr(card, 'title', 'Unknown'),
                    "items": getattr(card, 'items', [])
                }
                serialized_api_cards.append(card_dict)
        except Exception as card_error:
            logger.error(f"Failed to serialize card: {card_error}, card type: {type(card)}")
            continue
    
    api_cards = serialized_api_cards
    
    # Extract requires_action and action_data from final_state_raw (graph state)
    final_requires_action = None
    final_action_data = None
    if isinstance(final_state_raw, dict):
        final_requires_action = final_state_raw.get("requires_action")
        final_action_data = final_state_raw.get("action_data")
        
        # Debug logging for action data
        if final_requires_action:
            logger.info(f"Final requires_action: {final_requires_action}")
        if final_action_data:
            logger.info(f"Final action_data keys: {list(final_action_data.keys())}")
    
    # --- 6. å±¥æ­´ã®ä¿å­˜ ---
    # å±¥æ­´ä¿å­˜ã¯çµ±åˆãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§è‡ªå‹•å®Ÿè¡Œã•ã‚Œã‚‹

    # ç·Šæ€¥ãƒ¬ãƒ™ãƒ«ã®å¤‰æ›ï¼ˆæ•°å€¤ã«å¤‰æ›ï¼‰
    # final_stateãŒè¾æ›¸ã§ãªã„å ´åˆã®å¯¾å‡¦
    if isinstance(final_state, dict):
        # final_stateã«æ—¢ã«emergency_level_intãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        emergency_level_int = final_state.get("emergency_level_int")
        if emergency_level_int is None:
            emergency_level_int = _convert_emergency_level_to_int(final_state.get("emergency_level"))
    else:
        logger.error(f"final_state is not a dict, it's a {type(final_state)}")
        emergency_level_int = None
    
    # AgentResponseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    from app.schemas.common.enums import TaskType
    
    # current_task_typeã®å¤‰æ›
    if isinstance(final_state, dict):
        task_type_str = map_intent_to_task_type(final_state.get("session_info", {}).get("primary_intent", "unknown"))
        is_emergency_response = final_state.get("is_emergency_response", False)
        emergency_actions = final_state.get("emergency_actions")
        session_info = final_state.get("session_info", {})
    else:
        # final_stateãŒè¾æ›¸ã§ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        task_type_str = "unknown"
        is_emergency_response = False
        emergency_actions = None
        session_info = {}
    
    # emergency_infoã‹ã‚‰ã®å€¤ã‚‚è€ƒæ…®
    final_is_emergency = is_emergency_response or emergency_info["is_emergency"]
    final_emergency_actions = emergency_actions or emergency_info["emergency_actions"]
    final_emergency_level_int = emergency_level_int if emergency_level_int is not None else emergency_info["emergency_level"]
    
    try:
        current_task_type = TaskType(task_type_str)
    except ValueError:
        current_task_type = TaskType.UNKNOWN
    
    # --- çµ±åˆãƒ¡ãƒ¢ãƒªã‹ã‚‰æœ€çµ‚å±¥æ­´ã‚’å–å¾— ---
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã®ä¸¦åˆ—å‡¦ç†ï¼ˆ2å€é«˜é€ŸåŒ–ï¼‰
        async def update_firestore():
            return await memory_manager.update_firestore_with_new_message(
                session_id, device_identifier, 
                request.user_input, response_text_to_user
            )
        
        async def get_final_history():
            return await memory_manager.sync_histories(
                thread_id, session_id, device_identifier
            )
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        _, final_integrated_history = await asyncio.gather(
            update_firestore(),
            get_final_history(),
            return_exceptions=True
        )
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        if isinstance(final_integrated_history, Exception):
            logger.error(f"Failed to get final history: {final_integrated_history}")
            final_integrated_history = []
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«å¤‰æ›
        formatted_chat_history = memory_manager.format_for_response(final_integrated_history)
        
    except Exception as e_final_hist:
        logger.error(f"Failed to get final integrated history: {e_final_hist}")
        formatted_chat_history = []

    api_response = AgentResponse(
        response_text=response_text_to_user,
        current_task_type=current_task_type,
        status="success",
        generated_cards_for_frontend=api_cards,
        requires_action=final_requires_action,
        action_data=final_action_data,
        is_emergency_response=final_is_emergency,
        emergency_level=None,  # AgentResponseã§ã¯emergency_levelã¯EmergencyLevel enumãŒå¿…è¦ãªã®ã§Noneã«
        emergency_actions=final_emergency_actions,
        chat_history=formatted_chat_history,  # çµ±åˆã•ã‚ŒãŸå±¥æ­´ã‚’å«ã‚ã‚‹
        turn_count=session_info.get("turn_count"),
        debug_info={
            "final_task_type": task_type_str,
            "primary_intent": str(session_info.get("primary_intent", "unknown")),
            "elapsed_time_ms": (datetime.now(timezone.utc) - start_time_utc).total_seconds() * 1000,
            "emergency_level_int": final_emergency_level_int,  # æ•°å€¤ã¯debug_infoã«å«ã‚ã‚‹
            "memory_manager": memory_manager.get_thread_statistics()
        },
        session_id=session_id  # çµ±åˆãƒ¡ãƒ¢ãƒªã§æ±ºå®šã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ID
    )
    
    logger.info(f"API response prepared for session: {session_id}")
    return api_response

def _convert_emergency_level_to_int(level) -> Optional[int]:
    """Convert emergency level to int for API response."""
    if level is None:
        return None
    if isinstance(level, int):
        return level
    if isinstance(level, str):
        # String to int mapping
        level_map = {
            "normal": 0,
            "advisory": 1,
            "warning": 2,
            "critical": 3,
            "emergency": 4
        }
        return level_map.get(level.lower(), 0)
    return 0

def count_tokens_approximated(text: str) -> int:
    return len(text.split())

def map_response_type_to_task_type(response_type: str) -> str:
    """Map response_type from disaster analysis to valid TaskType."""
    response_type_mapping = {
        "educational_explanation": "disaster_related",
        "function_demonstration": "information_guide",
        "safety_status_check": "disaster_info",
        "hazard_map_display": "information_guide",
        "shelter_search": "evacuation_support",
        "information_lookup": "disaster_info",
        "guide_provision": "information_guide",
        "emergency_response": "emergency_response",
        "direct_answer": "disaster_related",
        "disaster_preparation": "disaster_preparation",  # Added mapping for disaster preparation
        # Add any other response_type values that might exist
        "general_knowledge_handler": "information_guide"  # Fallback for legacy values
    }
    
    # Return mapped value or default to "unknown" if not found
    return response_type_mapping.get(response_type, "unknown")

def map_intent_to_task_type(intent_value) -> str:
    """Map IntentCategory to TaskType string value."""
    # Convert enum to string if needed
    intent_str = str(intent_value) if hasattr(intent_value, 'value') else str(intent_value)
    
    # Map intent categories to task types
    # Support both underscore and non-underscore versions for compatibility
    intent_mapping = {
        # Basic intents
        "greeting": "greeting",
        "small_talk": "small_talk", 
        "off_topic": "off_topic",
        "unknown": "unknown",
        
        # Disaster-related intents (with multiple naming variations)
        "disaster_information": "disaster_info",
        "disaster_info_query": "disaster_info",
        "disaster_info": "disaster_info",
        
        "evacuation_support": "evacuation_support",
        "evacuation_support_request": "evacuation_support",
        "shelter_search": "evacuation_support",
        
        "emergency_help": "emergency_response",
        "emergency_help_request": "emergency_response",
        
        "disaster_preparation": "disaster_preparation",
        "disaster_preparation_guide": "disaster_preparation",
        
        "safety_confirmation": "safety_confirmation",
        "safety_confirmation_query": "safety_confirmation",
        
        "information_request": "information_guide",
        "communication_request": "communication",
    }
    
    return intent_mapping.get(intent_str, "unknown")

class SafetyBeaconOrchestrator:
    """SafetyBeaconã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹"""

    @classmethod
    async def process_request(cls, request: ChatRequest) -> AgentResponse:
        # Processing request via SafetyBeaconOrchestrator
        
        try:
            response = await run_agent_interaction(request)
            # run_agent_interactionãŒAgentResponseã‚’ç›´æ¥è¿”ã™ã‚ˆã†ã«ãªã£ãŸã®ã§ã€ãã®ã¾ã¾è¿”ã™
            if isinstance(response, AgentResponse):
                return response
            
            # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€è¾æ›¸ã®å ´åˆã¯å¤‰æ›
            return AgentResponse(
                response_text=response.get("response_text", response.get("responseText", "")),  # ä¸¡æ–¹ã®ã‚­ãƒ¼ã«å¯¾å¿œ
                current_task_type=response.get("debug_info", {}).get("final_task_type", "unknown"),
                requires_action=response.get("requires_action"),
                debug_info=response.get("debug_info", {}),
                generated_cards_for_frontend=response.get("generated_cards_for_frontend", []),
                # ç·Šæ€¥æ™‚å¿œç­”ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
                is_emergency_response=response.get("is_emergency_response", False),
                emergency_level=response.get("emergency_level"),
                emergency_actions=response.get("emergency_actions")
            )
        except Exception as e:
            logger.error(f"Error in SafetyBeaconOrchestrator: {str(e)}", exc_info=True)
            return AgentResponse(
                response_text=f"System error: {str(e)}",
                current_task_type="error",
                debug_info={
                    "error": str(e),
                    "conversation_id": getattr(request, "session_id", None)
                },
                is_emergency_response=False
            )

async def _detect_sms_intent_with_llm(user_input: str) -> bool:
    """LLMãƒ™ãƒ¼ã‚¹ã®SMSæ„å›³æ¤œå‡ºï¼ˆCLAUDE.mdåŸå‰‡æº–æ‹ ï¼‰"""
    try:
        if not user_input or len(user_input.strip()) < 5:
            return False
            
        from .llm_singleton import ainvoke_llm
        
        prompt = f"""Analyze if this user input expresses intent to send SMS/message for safety confirmation:

User input: "{user_input}"

Does this express intent to:
- Send safety confirmation to family/friends
- Contact someone about their safety status  
- Send message saying they are safe
- Notify others about their condition

Respond with only: true or false"""

        response = await ainvoke_llm(prompt, task_type="sms_intent_detection", temperature=0.1)
        return response.strip().lower() == "true"
        
    except Exception as e:
        logger.warning(f"LLM SMS intent detection failed: {e}")
        # CLAUDE.mdåŸå‰‡: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã¯ä½¿ç”¨ã—ãªã„
        # LLMåˆ¤å®šãŒå¤±æ•—ã—ãŸå ´åˆã¯Falseã‚’è¿”ã™ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
        return False

async def _generate_dynamic_sms_cards(
    intent: str, 
    entities: dict, 
    user_language: str,
    existing_cards: list
) -> list:
    """
    SMS functionality has been removed - handled by frontend
    """
    logger.info(f"SMS intent detected but functionality removed: {intent}")
    return existing_cards

async def _detect_emergency_content_semantic(response_text: str) -> bool:
    """çœŸã®LLMãƒ™ãƒ¼ã‚¹ã®ç·Šæ€¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡º"""
    try:
        from .llm_singleton import ainvoke_llm
        from app.prompts.disaster_prompts import EMERGENCY_CONTENT_DETECTION_PROMPT
        
        prompt = EMERGENCY_CONTENT_DETECTION_PROMPT.format(response_text=response_text[:300])
        
        result = await ainvoke_llm(prompt, task_type="content_analysis", temperature=0.1, max_tokens=10)
        return result.strip().lower() == "true"
    except:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã™
        return False

async def _determine_emergency_level_semantic(response_text: str) -> str:
    """çœŸã®LLMãƒ™ãƒ¼ã‚¹ã®ç·Šæ€¥ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
    try:
        from .llm_singleton import ainvoke_llm
        from app.prompts.disaster_prompts import EMERGENCY_LEVEL_ANALYSIS_PROMPT
        
        prompt = EMERGENCY_LEVEL_ANALYSIS_PROMPT.format(response_text=response_text[:300])
        
        result = await ainvoke_llm(prompt, task_type="emergency_level_analysis", temperature=0.1, max_tokens=10)
        level = result.strip().lower()
        return "critical" if level == "critical" else "warning"
    except:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«"warning"ã‚’è¿”ã™
        return "warning"
