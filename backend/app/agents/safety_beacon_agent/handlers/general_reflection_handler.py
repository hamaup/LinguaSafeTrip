"""
General Unified Reflection Handler
ã‚»ãƒ«ãƒ•ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ä»˜ãä¸€èˆ¬å¯¾å¿œãƒãƒ³ãƒ‰ãƒ©ãƒ¼
"""
import logging
import json
from typing import Dict, Any
from langchain_core.messages import AIMessage
from app.schemas.agent_state import AgentState
from ..core.llm_singleton import ainvoke_llm

logger = logging.getLogger(__name__)

async def general_unified_reflection(state: AgentState) -> Dict[str, Any]:
    """ã‚»ãƒ«ãƒ•ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ä»˜ãä¸€èˆ¬å¯¾å¿œãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    logger.info("ğŸ¤” NODE ENTRY: general_unified_reflection")
    
    user_input = state.get("user_input", "")
    user_language = state.get("user_language", "ja")
    reflection_count = state.get("reflection_count", 0)
    
    logger.info(f"ğŸ¤” Processing: '{user_input[:50]}...' (reflection: {reflection_count})")
    
    # ç¬¬1æ®µéš: åˆæœŸå¿œç­”ç”Ÿæˆ
    if reflection_count == 0:
        response = await _generate_initial_response(user_input, user_language)
        
        # ã‚»ãƒ«ãƒ•ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: ã“ã®å¿œç­”ã§ååˆ†ã‹ï¼Ÿ
        reflection_result = await _self_reflect(user_input, response, user_language)
        
        if reflection_result["needs_deeper_analysis"]:
            # ã‚ˆã‚Šæ·±ã„åˆ†æãŒå¿…è¦ - åŒã˜é–¢æ•°å†…ã§å‡¦ç†ã‚’ç¶™ç¶š
            logger.info("ğŸ¤” Needs deeper analysis - generating improved response")
            improved_response = await _generate_improved_response(
                user_input, response, reflection_result["feedback"], user_language
            )
            return _format_final_response(improved_response, user_language)
        else:
            # åˆå›å¿œç­”ã§ååˆ†
            logger.info("âœ… Initial response sufficient")
            return _format_final_response(response, user_language)
    
    # ç¬¬2æ®µéš: æ·±ã„åˆ†æå¾Œã®å¿œç­”
    elif reflection_count == 1:
        initial_response = state.get("initial_response", "")
        feedback = state.get("reflection_feedback", "")
        
        logger.info("ğŸ§  Generating improved response after reflection")
        improved_response = await _generate_improved_response(
            user_input, initial_response, feedback, user_language
        )
        
        return _format_final_response(improved_response, user_language)
    
    # æœ€å¤§2å›ã¾ã§
    else:
        logger.warning("ğŸš« Max reflections reached, using fallback")
        fallback_response = await _generate_fallback_response(user_input, user_language)
        return _format_final_response(fallback_response, user_language)

async def _generate_initial_response(user_input: str, user_language: str) -> str:
    """åˆæœŸå¿œç­”ç”Ÿæˆ"""
    
    prompt = f"""You are SafetyBee, a disaster prevention assistant. A user asked something that may not be directly disaster-related.

User request: "{user_input}"
Response language: {user_language}

Generate a helpful initial response that:
1. Acknowledges their request
2. Gently guides them toward disaster preparedness if possible
3. Offers relevant SafetyBee features

Keep it friendly and helpful, around 2-3 sentences."""

    try:
        response = await ainvoke_llm(
            prompt,
            task_type="general_initial_response",
            temperature=0.7,
            max_tokens=200
        )
        return response.strip()
    except Exception as e:
        logger.error(f"Initial response generation failed: {e}")
        return _get_fallback_message(user_language)

async def _self_reflect(user_input: str, response: str, user_language: str) -> Dict[str, Any]:
    """ã‚»ãƒ«ãƒ•ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åˆ†æ"""
    
    reflection_prompt = f"""Analyze if this response adequately addresses the user's needs:

User: "{user_input}"
Current Response: "{response}"

Consider:
1. Could this request be related to disaster preparedness?
2. Is there a hidden safety concern?
3. Can we provide more helpful guidance?
4. Should we dig deeper into their actual needs?
5. Are there relevant SafetyBee features we should mention?

Return JSON:
{{
    "needs_deeper_analysis": true/false,
    "feedback": "specific improvement suggestions",
    "potential_safety_angle": "if any safety relevance found",
    "suggested_features": ["list of relevant SafetyBee features"]
}}"""
    
    try:
        result = await ainvoke_llm(
            reflection_prompt,
            task_type="reflection_analysis", 
            temperature=0.3,
            max_tokens=300
        )
        return json.loads(result.strip())
    except Exception as e:
        logger.error(f"Self reflection failed: {e}")
        return {
            "needs_deeper_analysis": False,
            "feedback": "Reflection failed, using initial response",
            "potential_safety_angle": "",
            "suggested_features": []
        }

async def _generate_improved_response(
    user_input: str, 
    initial_response: str, 
    feedback: str, 
    user_language: str
) -> str:
    """æ”¹å–„ã•ã‚ŒãŸå¿œç­”ç”Ÿæˆ"""
    
    prompt = f"""Based on reflection feedback, generate an improved response.

User request: "{user_input}"
Initial response: "{initial_response}"
Reflection feedback: "{feedback}"
Response language: {user_language}

Generate an improved response that:
1. Addresses the feedback points
2. Makes stronger connections to disaster preparedness
3. Suggests specific SafetyBee features
4. Maintains a helpful and engaging tone

Keep it concise but more comprehensive than the initial response."""

    try:
        response = await ainvoke_llm(
            prompt,
            task_type="improved_response",
            temperature=0.7,
            max_tokens=300
        )
        return response.strip()
    except Exception as e:
        logger.error(f"Improved response generation failed: {e}")
        return initial_response  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

async def _generate_fallback_response(user_input: str, user_language: str) -> str:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ç”Ÿæˆ"""
    
    # English-only fallback message (per CLAUDE.md principles)
    return "I apologize, but I cannot answer that question. SafetyBee is a disaster prevention app. Please use our shelter search, disaster information, or preparedness guide features."

def _get_fallback_message(user_language: str) -> str:
    """æœ€åŸºæœ¬çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    
    # English-only fallback message (per CLAUDE.md principles)
    return "Thank you for using SafetyBee. Please ask me about disaster preparedness."

def _format_final_response(response_text: str, user_language: str) -> Dict[str, Any]:
    """æœ€çµ‚å¿œç­”ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    
    # åŸºæœ¬çš„ãªæ©Ÿèƒ½ç´¹ä»‹ã‚«ãƒ¼ãƒ‰
    suggestion_cards = [
        {
            "card_type": "app_feature_recommendation",
            "card_id": "evacuation_search",
            "title": "é¿é›£æ‰€æ¤œç´¢" if user_language == "ja" else "Shelter Search",
            "action_query": "æœ€å¯„ã‚Šã®é¿é›£æ‰€ã‚’æ•™ãˆã¦" if user_language == "ja" else "Find nearest shelter"
        },
        {
            "card_type": "app_feature_recommendation", 
            "card_id": "disaster_info",
            "title": "ç½å®³æƒ…å ±" if user_language == "ja" else "Disaster Info",
            "action_query": "ç¾åœ¨ã®ç½å®³æƒ…å ±ã‚’æ•™ãˆã¦" if user_language == "ja" else "Show current disaster information"
        },
        {
            "card_type": "app_feature_recommendation",
            "card_id": "preparedness_guide", 
            "title": "é˜²ç½ã‚¬ã‚¤ãƒ‰" if user_language == "ja" else "Preparedness Guide",
            "action_query": "é˜²ç½ã®æº–å‚™ã«ã¤ã„ã¦æ•™ãˆã¦" if user_language == "ja" else "Tell me about disaster preparedness"
        }
    ]
    
    message = AIMessage(
        content=response_text,
        additional_kwargs={
            "cards": suggestion_cards[:2],  # æœ€å¤§2æš
            "handler_type": "general_reflection",
            "reflection_used": True
        }
    )
    
    return {
        "messages": [message],
        "final_response_text": response_text,
        "last_response": response_text,
        "cards_to_display_queue": suggestion_cards[:2],
        "current_task_type": ["general_inquiry_with_reflection"],
        "handler_completed": True
    }