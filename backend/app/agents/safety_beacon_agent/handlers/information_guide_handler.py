import logging
import uuid
import json
import re
import asyncio
from typing import Dict, Any, List, Optional, cast
from functools import lru_cache

from app.schemas.agent import AgentState, SuggestionCard, SuggestionCardActionButton
from app.schemas.guide import GuideContent # GuideContentã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from app.schemas.search_results import SearchResultItem # SearchResultItemã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from app.tools.guide_tools import UnifiedGuideSearchTool # æ–°ã—ã„UnifiedGuideSearchToolã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from app.tools.web_search_tools import get_web_search_tool # Get appropriate web search tool

# ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from ..core.llm_singleton import ainvoke_llm # çµ±ä¸€çš„ãªLLMå‘¼ã³å‡ºã—
from app.prompts.prompts import SYSTEM_PROMPT_TEXT, INFORMATION_GUIDE_RESPONSE_PROMPT_TEMPLATE, SUGGESTION_CARD_GENERATION_PROMPT_TEMPLATE # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain_core.messages import SystemMessage, HumanMessage # LangChainãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ


logger = logging.getLogger(__name__)

# Import TTL cache
from app.utils.ttl_cache import TTLCache
from app.agents.safety_beacon_agent.handlers.complete_response_handlers import CompleteResponseGenerator

# Translation cache with TTL (24 hours, max 5000 entries)
_translation_cache = TTLCache(
    name="translation_cache",
    default_ttl_seconds=86400,  # 24 hours
    max_size=5000,
    cleanup_interval_seconds=3600  # cleanup every hour
)

# ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°: ãƒãƒƒãƒå‡¦ç†ã®æœ‰åŠ¹/ç„¡åŠ¹
USE_BATCH_PROCESSING = True

async def _get_cached_japanese_query(query: str, search_type: str) -> str:
    """
    Get Japanese translation of query with caching to reduce translation overhead
    """
    # More accurate Japanese detection (exclude Chinese-only characters)
    # Check for hiragana or katakana which are unique to Japanese
    is_japanese = bool(re.search(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]', query))
    
    if is_japanese:
        return query
    
    # Check cache first
    cache_key = TTLCache.make_key(query, search_type)
    cached_result = _translation_cache.get(cache_key)
    if cached_result is not None:
        logger.info(f"ğŸ”„ Using cached Japanese translation for {search_type}: '{query}' -> '{cached_result}'")
        return cached_result
    
    # Translate and cache
    try:
        translation_prompt = f"""Translate the following query to natural Japanese for {search_type}:

Query: "{query}"

IMPORTANT: Return ONLY the Japanese translation, no explanations or options.

Japanese translation:"""
        translation_response = await ainvoke_llm(translation_prompt, task_type="translation", temperature=0.3, max_tokens=100)
        # Extract just the Japanese text if LLM returns explanations
        japanese_query = translation_response.strip()
        # Clean up common patterns where LLM might return extra text
        if "**" in japanese_query:
            # Extract text between ** markers
            match = re.search(r'\*\*([^*]+)\*\*', japanese_query)
            if match:
                japanese_query = match.group(1)
        # If response contains multiple lines, take the first Japanese line
        lines = japanese_query.split('\n')
        for line in lines:
            if any(char in line for char in 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ãŒããã’ã”ã•ã—ã™ã›ãã–ã˜ãšãœããŸã¡ã¤ã¦ã¨ã ã¢ã¥ã§ã©ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“'):
                japanese_query = line.strip()
                break
        
        # Cache the result with TTL
        _translation_cache.set(cache_key, japanese_query)
        return japanese_query
    except Exception as e:
        logger.warning(f"{search_type} query translation failed: {e}, using original query")
        return query

def generate_emotional_support_response(emotional_context: Dict[str, Any], user_language: str, query_type: str) -> str:
    """
    æƒ…å ±ã‚¬ã‚¤ãƒ‰ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç”¨ã®æ„Ÿæƒ…çš„ã‚µãƒãƒ¼ãƒˆå¿œç­”ç”Ÿæˆ
    
    NOTE: This function uses predefined templates instead of LLM.
    TODO: Replace with LLM-based generation following CLAUDE.md principles.
    
    Args:
        emotional_context: extract_emotional_context()ã®çµæœ
        user_language: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨€èª
        query_type: ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ— ("general", "disaster", etc.)
    
    Returns:
        å…±æ„Ÿçš„ã§æ”¯æ´çš„ãªå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    logger.info(f"ğŸ«‚ Information Guide - Generating emotional support response for {emotional_context['emotional_state']}")
    
    emotional_state = emotional_context.get('emotional_state', 'anxious')
    intensity = emotional_context.get('intensity', 1)
    support_level = emotional_context.get('support_level', 'moderate')
    
    # è¨€èªåˆ¥ã®å…±æ„Ÿçš„é–‹å§‹ãƒ•ãƒ¬ãƒ¼ã‚º
    empathy_starters = {
        'ja': {
            'anxious': 'ãŠæ°—æŒã¡ã¨ã¦ã‚‚ã‚ˆãã‚ã‹ã‚Šã¾ã™ã€‚',
            'scared': 'ãŠæ°—æŒã¡ãŠå¯Ÿã—ã—ã¾ã™ã€‚',
            'worried': 'ã”å¿ƒé…ãªãŠæ°—æŒã¡ã€ã‚ˆãã‚ã‹ã‚Šã¾ã™ã€‚',
            'stressed': 'ãŠç–²ã‚Œã•ã¾ã§ã™ã€‚å¤§å¤‰ãªçŠ¶æ³ã§ã™ã­ã€‚'
        },
        'en': {
            'anxious': 'I completely understand how you\'re feeling.',
            'scared': 'I can sense your fear, and that\'s completely natural.',
            'worried': 'Your worries are completely understandable.',
            'stressed': 'I can see you\'re going through a tough time.'
        }
    }
    
    # è¨€èªåˆ¥ã®å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹ä¸­é–“éƒ¨åˆ†
    reassurance_middle = {
        'ja': {
            'disaster': 'ç½å®³ã«ã¤ã„ã¦å¿ƒé…ã«ãªã‚‹ã®ã¯ã€ã¨ã¦ã‚‚è‡ªç„¶ãªã“ã¨ã§ã™ã€‚ã‚ãªãŸã¯ä¸€äººã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚',
            'general': 'ä¸å®‰ã«æ„Ÿã˜ã‚‹ã“ã¨ã¯è‡ªç„¶ãªã“ã¨ã§ã™ã€‚ä¸€ç·’ã«è€ƒãˆã¦ã„ãã¾ã—ã‚‡ã†ã€‚'
        },
        'en': {
            'disaster': 'It\'s completely natural to worry about disasters. You\'re not alone in feeling this way.',
            'general': 'It\'s natural to feel anxious. Let\'s work through this together.'
        }
    }
    
    # è¨€èªåˆ¥ã®åŠ±ã¾ã—ã®çµ‚äº†éƒ¨åˆ†
    encouragement_endings = {
        'ja': {
            'light': 'ç§ãŒã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã®ã§ã€ä¸€ç·’ã«è€ƒãˆã¦ã„ãã¾ã—ã‚‡ã†ã€‚',
            'moderate': 'ä¸€ç·’ã«æº–å‚™ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ãã£ã¨å¤§ä¸ˆå¤«ã§ã™ã€‚',
            'strong': 'ç§ãŒå…¨åŠ›ã§ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ã„ã¤ã§ã‚‚ãŠå£°ã‹ã‘ãã ã•ã„ã€‚',
            'crisis': 'ä»Šã™ãã‚µãƒãƒ¼ãƒˆãŒå¿…è¦ã§ã™ã­ã€‚ç§ãŒãŠæ‰‹ä¼ã„ã—ã¾ã™ã€‚å®‰å¿ƒã—ã¦ãã ã•ã„ã€‚'
        },
        'en': {
            'light': 'I\'m here to support you. Let\'s work through this together.',
            'moderate': 'We\'ll prepare together step by step. You\'ve got this.',
            'strong': 'I\'m here to fully support you. Please reach out anytime.',
            'crisis': 'You need support right now, and I\'m here to help. You\'re safe.'
        }
    }
    
    # å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹éƒ¨åˆ†
    practical_advice = {
        'ja': {
            'disaster': 'ä¸å®‰ãªæ™‚ã“ãã€ã§ãã‚‹ã“ã¨ã‹ã‚‰ä¸€ã¤ãšã¤å§‹ã‚ã¦ã„ãã¾ã—ã‚‡ã†ï¼š\\n\\nâ€¢ ä»Šã®å®‰å…¨ã‚’ç¢ºèªã™ã‚‹\\nâ€¢ å¿…è¦ãªæƒ…å ±ã‚’æ•´ç†ã™ã‚‹\\nâ€¢ å…·ä½“çš„ãªæº–å‚™ã‚’å°‘ã—ãšã¤é€²ã‚ã‚‹',
            'general': 'å¿ƒé…äº‹ãŒã‚ã‚‹ã¨ãã¯ä»¥ä¸‹ã®ã“ã¨ã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼š\\n\\nâ€¢ æ·±å‘¼å¸ã‚’ã—ã¦è½ã¡ç€ã\\nâ€¢ å…·ä½“çš„ãªå•é¡Œã‚’æ•´ç†ã™ã‚‹\\nâ€¢ ä¸€æ­©ãšã¤è§£æ±ºç­–ã‚’è€ƒãˆã‚‹'
        },
        'en': {
            'disaster': 'When we\'re anxious, taking small steps can help:\\n\\nâ€¢ Check your current safety\\nâ€¢ Gather reliable information\\nâ€¢ Make preparations step by step',
            'general': 'When you\'re worried, try these steps:\\n\\nâ€¢ Take deep breaths to calm down\\nâ€¢ Organize your specific concerns\\nâ€¢ Think through solutions step by step'
        }
    }
    
    # è¨€èªã¨ã‚µãƒãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦å¿œç­”ã‚’æ§‹ç¯‰
    lang_key = user_language if user_language in empathy_starters else 'en'
    advice_key = query_type if query_type in reassurance_middle[lang_key] else 'general'
    
    # å…±æ„Ÿçš„é–‹å§‹
    starter = empathy_starters[lang_key].get(emotional_state, empathy_starters[lang_key]['anxious'])
    
    # å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹ä¸­é–“éƒ¨
    middle = reassurance_middle[lang_key][advice_key]
    
    # å®Ÿç”¨çš„ã‚¢ãƒ‰ãƒã‚¤ã‚¹
    advice = practical_advice[lang_key][advice_key]
    
    # åŠ±ã¾ã—ã®çµ‚äº†
    ending = encouragement_endings[lang_key][support_level]
    
    # å¿œç­”ã‚’çµ„ã¿ç«‹ã¦
    response = f"{starter}\\n\\n{middle}\\n\\n{advice}\\n\\n{ending}"
    
    # Information Guide - Generated emotional support response
    
    return response

async def _invoke_llm_for_task_specific_processing(
    task_prompt_template: str, # ã‚¿ã‚¹ã‚¯ç‰¹æœ‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    user_language: str,
    data_to_process: Dict[str, Any],
    user_input: str = ""
) -> Dict[str, Any]:
    """
    ç‰¹å®šã®æƒ…å ±å‡¦ç†ã‚¿ã‚¹ã‚¯ï¼ˆã‚¬ã‚¤ãƒ‰è¦ç´„ã€Webæ¤œç´¢çµæœã®æ•´å½¢ãªã©ï¼‰ã®ãŸã‚ã«LLMã‚’å‘¼ã³å‡ºã™ã€‚
    SYSTEM_PROMPT_TEXTã®é–¢é€£æŒ‡ç¤ºã¨ã€å‡¦ç†å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ„ã¿ç«‹ã¦
    # SYSTEM_PROMPT_TEXTã¯LLMã®å…¨ä½“çš„ãªæŒ¯ã‚‹èˆã„ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã€å¸¸ã«å«ã‚ã‚‹
    # task_prompt_templateã¯ã€å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯æŒ‡ç¤ºã¨ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€
    # HttpUrlå‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    def convert_httpurl_to_str(obj):
        if isinstance(obj, list):
            return [convert_httpurl_to_str(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: convert_httpurl_to_str(v) for k, v in obj.items()}
        # Pydantic HttpUrlå‹ã‹ã©ã†ã‹ã‚’ã‚ˆã‚Šç¢ºå®Ÿã«åˆ¤å®š
        elif obj.__class__.__name__ == 'HttpUrl' and hasattr(obj, 'scheme'):
            return str(obj)
        elif isinstance(obj, (str, int, float, bool)) or obj is None:
            return obj
        # ãã®ä»–ã®å‹ã¯ãã®ã¾ã¾ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®å‹å¤‰æ›ã‚’å®Ÿè£…ï¼‰
        try:
            # äºˆæœŸã—ãªã„å‹ã®å ´åˆã€æ–‡å­—åˆ—å¤‰æ›ã‚’è©¦ã¿ã‚‹
            return str(obj)
        except Exception:
            return obj # å¤‰æ›ã§ããªã‘ã‚Œã°ãã®ã¾ã¾è¿”ã™

    processed_data = convert_httpurl_to_str(data_to_process)

    full_prompt_content = task_prompt_template.format(
        user_language=user_language,
        user_input=user_input,
        data_to_process=json.dumps(processed_data, ensure_ascii=False, indent=2)
    )

    messages = [
        SystemMessage(content=SYSTEM_PROMPT_TEXT.format(user_language=user_language)), # SYSTEM_PROMPT_TEXTã‚‚user_languageã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        HumanMessage(content=full_prompt_content)
    ]

    raw_llm_output = await ainvoke_llm(messages, task_type="information_guide", max_tokens=8000)

    response_text_for_user = raw_llm_output
    suggestion_card_data = None

    try:
        # LLMãŒ {"responseText": "...", "card": {...}} ã®ã‚ˆã†ãªJSON/dictã‚’è¿”ã™ã¨æœŸå¾…
        if isinstance(raw_llm_output, dict):
            parsed_llm_json = raw_llm_output
        else:
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å»
            json_text = raw_llm_output.strip()
            if json_text.startswith('```json'):
                json_text = json_text[7:].rstrip('```').strip()
            elif json_text.startswith('```'):
                json_text = json_text[3:].rstrip('```').strip()
            
            # JSONãƒ‘ãƒ¼ã‚¹è©¦è¡Œ
            try:
                parsed_llm_json = json.loads(json_text)
            except json.JSONDecodeError as e:
                # ã‚ˆã‚Šå …ç‰¢ãªJSONä¿®æ­£ã‚’è©¦è¡Œ
                fixed_json = json_text
                
                # æ”¹è¡Œæ–‡å­—ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ï¼ˆæ–‡å­—åˆ—å†…ã®æ”¹è¡Œã¯ä¿æŒï¼‰
                import re
                # æ–‡å­—åˆ—å¤–ã®æ”¹è¡Œã®ã¿ç½®æ›
                parts = re.split(r'("(?:[^"\\]|\\.)*")', fixed_json)
                for i in range(0, len(parts), 2):  # å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯æ–‡å­—åˆ—å¤–
                    parts[i] = parts[i].replace('\n', ' ').replace('\t', ' ')
                fixed_json = ''.join(parts)
                
                # æœ«å°¾ã‚«ãƒ³ãƒã®é™¤å»
                fixed_json = re.sub(r',\s*}', '}', fixed_json)
                fixed_json = re.sub(r',\s*]', ']', fixed_json)
                
                # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ãªã„å¼•ç”¨ç¬¦ã®ä¿®æ­£
                fixed_json = re.sub(r'(?<!\\)"([^"]*?)(?<!\\)"([^":,}\]]*?)(?<!\\)"', r'"\1\2"', fixed_json)
                
                # ä¸å®Œå…¨ãªJSONã®å ´åˆã€çµ‚äº†ã‚’è£œå®Œ
                open_braces = fixed_json.count('{') - fixed_json.count('}')
                if open_braces > 0:
                    fixed_json += '}' * open_braces
                open_brackets = fixed_json.count('[') - fixed_json.count(']')
                if open_brackets > 0:
                    fixed_json += ']' * open_brackets
                
                try:
                    parsed_llm_json = json.loads(fixed_json)
                except json.JSONDecodeError:
                    # æœ€å¾Œã®æ‰‹æ®µï¼šJSONã®ä¸€éƒ¨ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                    # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸå¼•ç”¨ç¬¦ã‚’è€ƒæ…®ã—ãŸæ­£è¦è¡¨ç¾
                    json_match = re.search(r'"responseText"\s*:\s*"((?:[^"\\]|\\.)*)"', fixed_json, re.DOTALL)
                    if json_match:
                        response_text_for_user = json_match.group(1)
                        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                        response_text_for_user = response_text_for_user.replace('\\"', '"').replace('\\n', '\n').replace('\\\\', '\\')
                        logger.warning(f"Extracted responseText from malformed JSON: {response_text_for_user[:100]}...")
                        return {
                            "processed_text_for_user": response_text_for_user,
                            "suggestion_card_data": None
                        }
                    else:
                        raise e

        if isinstance(parsed_llm_json, dict):
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœŸå¾…å½¢å¼ã«åˆã‚ã›ã¦ä¿®æ­£
            response_text_for_user = parsed_llm_json.get("responseText",
                                                       parsed_llm_json.get("processed_text_for_user", raw_llm_output))
            suggestion_card_data = parsed_llm_json.get("card",
                                                     parsed_llm_json.get("suggestion_card_data"))
    except (json.JSONDecodeError, TypeError) as e:
        logger.warning(f"LLM output parsing failed after cleanup attempts: {e}. Using raw output as text.")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”Ÿã®LLMå‡ºåŠ›ã‚’ä½¿ç”¨
        # ãŸã ã—ã€guide_contentãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ç›´æ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        if isinstance(data_to_process, dict) and "guide_content" in data_to_process:
            guide_content = data_to_process["guide_content"]
            if guide_content and isinstance(guide_content, list) and len(guide_content) > 0:
                # ã‚¬ã‚¤ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ç›´æ¥å¿œç­”ã‚’æ§‹ç¯‰ï¼ˆè¤‡æ•°ã®çµæœã‚’çµ±åˆï¼‰
                all_parts = []
                for idx, content in enumerate(guide_content[:3]):  # æœ€å¤§3ä»¶ã¾ã§å‡¦ç†
                    if isinstance(content, dict):
                        title = content.get("title", "")
                        description = content.get("description", "")
                        content_text = content.get("content", "")
                        
                        # å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
                        response_parts = []
                        if title:
                            response_parts.append(f"**{title}**")
                        if description:
                            response_parts.append(description)
                        if content_text:
                            # ãƒ¢ãƒã‚¤ãƒ«ç”¨ã«çŸ­ç¸®ï¼ˆé‡è¦éƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼‰
                            if len(content_text) > 200:
                                content_text = content_text[:200] + "..."
                            response_parts.append(content_text)
                        
                        if response_parts:
                            all_parts.append("\n\n".join(response_parts))
                
                if all_parts:
                    response_text_for_user = "\n\n---\n\n".join(all_parts)
                    logger.info(f"Fallback: Constructed response from {len(all_parts)} guide contents")

    return {
        "processed_text_for_user": response_text_for_user,
        "suggestion_card_data": suggestion_card_data
    }


def _get_default_guide_data(guide_type: str) -> Dict[str, Any]:
    """Return default guide data when mock files don't exist"""
    default_guides = {
        "emergency_kit": {
            "en": {
                "content": "Essential Emergency Kit Items:\nâ€¢ Water (1 gallon per person per day for 3 days)\nâ€¢ Non-perishable food (3-day supply)\nâ€¢ Battery-powered or hand crank radio\nâ€¢ Flashlight and extra batteries\nâ€¢ First aid kit\nâ€¢ Whistle for signaling\nâ€¢ Face masks\nâ€¢ Medications\nâ€¢ Important documents\nâ€¢ Cash and credit cards"
            },
            "ja": {
                "content": "é˜²ç½ã‚°ãƒƒã‚ºã®åŸºæœ¬ãƒªã‚¹ãƒˆï¼š\nâ€¢ æ°´ï¼ˆ1äºº1æ—¥3ãƒªãƒƒãƒˆãƒ«ã€3æ—¥åˆ†ï¼‰\nâ€¢ éå¸¸é£Ÿï¼ˆ3æ—¥åˆ†ï¼‰\nâ€¢ æºå¸¯ãƒ©ã‚¸ã‚ªï¼ˆé›»æ± å¼ã¾ãŸã¯æ‰‹å›ã—å¼ï¼‰\nâ€¢ æ‡ä¸­é›»ç¯ã¨äºˆå‚™é›»æ± \nâ€¢ æ•‘æ€¥ã‚»ãƒƒãƒˆ\nâ€¢ ãƒ›ã‚¤ãƒƒã‚¹ãƒ«\nâ€¢ ãƒã‚¹ã‚¯\nâ€¢ å¸¸å‚™è–¬\nâ€¢ é‡è¦æ›¸é¡\nâ€¢ ç¾é‡‘ã¨ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰"
            }
        },
        "typhoon_preparation": {
            "en": {
                "content": "Typhoon Preparation:\nâ€¢ Secure outdoor items\nâ€¢ Stock up on water and food\nâ€¢ Charge all devices\nâ€¢ Fill bathtub with water\nâ€¢ Know evacuation routes\nâ€¢ Have emergency contacts ready"
            },
            "ja": {
                "content": "å°é¢¨å¯¾ç­–ï¼š\nâ€¢ å±‹å¤–ã®ç‰©ã‚’å›ºå®šãƒ»åç´\nâ€¢ æ°´ã¨é£Ÿæ–™ã®å‚™è“„\nâ€¢ å…¨ã¦ã®æ©Ÿå™¨ã‚’å……é›»\nâ€¢ æµ´æ§½ã«æ°´ã‚’è²¯ã‚ã‚‹\nâ€¢ é¿é›£çµŒè·¯ã®ç¢ºèª\nâ€¢ ç·Šæ€¥é€£çµ¡å…ˆã®æº–å‚™"
            }
        },
        "earthquake_preparation": {
            "en": {
                "content": "Earthquake Preparation:\nâ€¢ Secure furniture to walls\nâ€¢ Know Drop, Cover, Hold On\nâ€¢ Identify safe spots in each room\nâ€¢ Practice evacuation drills\nâ€¢ Keep shoes by bedside\nâ€¢ Store emergency supplies"
            },
            "ja": {
                "content": "åœ°éœ‡å¯¾ç­–ï¼š\nâ€¢ å®¶å…·ã®å›ºå®š\nâ€¢ DROP, COVER, HOLD ONã‚’è¦šãˆã‚‹\nâ€¢ å„éƒ¨å±‹ã®å®‰å…¨ãªå ´æ‰€ã‚’ç¢ºèª\nâ€¢ é¿é›£è¨“ç·´ã®å®Ÿæ–½\nâ€¢ æ•å…ƒã«é´ã‚’æº–å‚™\nâ€¢ é˜²ç½ç”¨å“ã®å‚™è“„"
            }
        }
    }
    
    # Return requested guide or emergency_kit as default
    return default_guides.get(guide_type, default_guides["emergency_kit"])


async def _get_mock_preparation_guide(query: str, language: str) -> str:
    """
    Get mock preparation guide for debug/test mode using LLM-based selection
    """
    import json
    import os
    
    # Use LLM to determine which mock guide is most relevant
    prompt = f"""Analyze this disaster preparation query and determine the most relevant guide type.

Query: "{query}"

Available guide types:
- emergency_kit: General emergency supplies and disaster kit
- typhoon_preparation: Typhoon/hurricane specific preparation
- earthquake_preparation: Earthquake specific preparation

Return ONLY the guide type ID that best matches the query."""
    
    try:
        guide_type = await ainvoke_llm(prompt, task_type="analysis", temperature=0.3, max_tokens=50)
        guide_type = guide_type.strip().lower()
        
        # Mock data files don't exist - return default guide content
        guide_data = _get_default_guide_data(guide_type)
            
        # Return content in requested language
        lang_data = guide_data.get(language, guide_data.get('en', {}))
        return lang_data.get('content', 'No preparation guide available.')
        
    except Exception as e:
        logger.error(f"Error loading mock preparation guide: {e}")
        # Fallback content
        if language == "ja":
            return "é˜²ç½æº–å‚™ã‚¬ã‚¤ãƒ‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        else:
            return "Failed to load preparation guide."

async def _extract_disaster_type_from_query(query: str) -> str:
    """
    Extract disaster type from user query using LLM
    
    Args:
        query: User input query
        
    Returns:
        Disaster type (typhoon, earthquake, tsunami, flood, etc.) or 'general'
    """
    try:
        prompt = f"""Analyze the following query and identify the disaster type being asked about.

Query: "{query}"

Return ONLY one of these disaster types:
- typhoon
- earthquake
- tsunami
- flood
- wildfire
- volcanic_eruption
- heavy_rain
- general (if no specific disaster type is mentioned)

Disaster type:"""
        
        response = await ainvoke_llm(prompt, task_type="classification", temperature=0.3)
        disaster_type = response.strip().lower()
        
        # Validate response
        valid_types = {'typhoon', 'earthquake', 'tsunami', 'flood', 'wildfire', 'volcanic_eruption', 'heavy_rain', 'general'}
        if disaster_type not in valid_types:
            logger.warning(f"Invalid disaster type extracted: {disaster_type}, defaulting to 'general'")
            return 'general'
            
        return disaster_type
        
    except Exception as e:
        logger.error(f"Failed to extract disaster type: {e}")
        return 'general'


async def _generate_context_aware_fallback(disaster_type: str, user_language: str) -> str:
    """
    Generate context-aware fallback response based on disaster type
    
    Args:
        disaster_type: Type of disaster (typhoon, earthquake, etc.)
        user_language: User's language code
        
    Returns:
        Context-appropriate safety information in English (will be translated by response_generator)
    """
    
    # Define disaster-specific safety information
    disaster_info = {
        'typhoon': {
            'title': 'Typhoon Preparation',
            'content': """Here are essential typhoon preparation steps:

**Before the Typhoon:**
â€¢ Secure outdoor items that could become projectiles
â€¢ Stock up on water, food, and emergency supplies
â€¢ Charge all devices and prepare battery backups
â€¢ Board up windows or use storm shutters
â€¢ Fill bathtubs and containers with water

**During the Typhoon:**
â€¢ Stay indoors away from windows
â€¢ Monitor official weather updates
â€¢ Be ready to move to higher floors if flooding occurs
â€¢ Never go outside during the eye of the storm

**Emergency Kit Essentials:**
â€¢ Water (3 days supply)
â€¢ Non-perishable food
â€¢ Flashlights and batteries
â€¢ First aid kit
â€¢ Important documents in waterproof container"""
        },
        'earthquake': {
            'title': 'Earthquake Safety',
            'content': """Here are essential earthquake safety actions:

**During Earthquakes:**
â€¢ Drop, Cover, and Hold On
â€¢ Stay away from windows and heavy objects
â€¢ If outdoors, move to open space away from buildings
â€¢ If driving, stop safely and stay in vehicle

**After Earthquakes:**
â€¢ Check for injuries and damage
â€¢ Be prepared for aftershocks
â€¢ Listen to official information
â€¢ Evacuate if building is damaged"""
        },
        'tsunami': {
            'title': 'Tsunami Safety',
            'content': """Here are essential tsunami safety actions:

**During Tsunami Warnings:**
â€¢ Move immediately to high ground or inland
â€¢ Never wait to see the wave
â€¢ Stay away from the coast
â€¢ Follow marked evacuation routes

**Important Rules:**
â€¢ A small tsunami at one point can be large elsewhere
â€¢ Tsunamis can continue for hours
â€¢ Never return until officials say it's safe"""
        },
        'flood': {
            'title': 'Flood Safety',
            'content': """Here are essential flood safety actions:

**Before Flooding:**
â€¢ Monitor weather alerts
â€¢ Prepare to evacuate quickly
â€¢ Move valuables to higher floors
â€¢ Turn off utilities if instructed

**During Flooding:**
â€¢ Never walk or drive through flood waters
â€¢ Move to higher ground immediately
â€¢ Avoid contact with floodwater
â€¢ Stay informed through official channels"""
        },
        'wildfire': {
            'title': 'Wildfire Safety',
            'content': """Here are essential wildfire safety actions:

**If Evacuation is Ordered:**
â€¢ Leave immediately
â€¢ Close all windows and doors
â€¢ Turn off gas and propane
â€¢ Take emergency supplies and documents

**Evacuation Preparation:**
â€¢ Keep car fueled and facing out
â€¢ Have multiple evacuation routes
â€¢ Stay informed about fire conditions"""
        },
        'volcanic_eruption': {
            'title': 'Volcanic Eruption Safety',
            'content': """Here are essential volcanic eruption safety actions:

**During Eruption:**
â€¢ Follow evacuation orders immediately
â€¢ Protect yourself from ash fall
â€¢ Stay indoors with windows and doors closed
â€¢ Wear masks or breathe through cloth

**Important Precautions:**
â€¢ Avoid low-lying areas
â€¢ Stay away from lava flows
â€¢ Be aware of mudflows in valleys"""
        },
        'heavy_rain': {
            'title': 'Heavy Rain Safety',
            'content': """Here are essential heavy rain safety actions:

**During Heavy Rain:**
â€¢ Avoid flooded areas and underpasses
â€¢ Stay away from rivers and streams
â€¢ Be alert for landslide risks
â€¢ Monitor weather updates

**Safety Measures:**
â€¢ Never drive through flooded roads
â€¢ Move to higher ground if needed
â€¢ Prepare for power outages
â€¢ Keep emergency supplies ready"""
        },
        'general': {
            'title': 'General Emergency Preparedness',
            'content': """Here are general emergency preparedness guidelines:

**Emergency Kit Essentials:**
â€¢ Water (1 gallon per person per day)
â€¢ Non-perishable food (3-day supply)
â€¢ Battery-powered radio
â€¢ Flashlight and extra batteries
â€¢ First aid kit
â€¢ Whistle for signaling
â€¢ Local maps

**Important Actions:**
â€¢ Know your evacuation routes
â€¢ Have a family communication plan
â€¢ Keep important documents safe
â€¢ Stay informed through official channels"""
        }
    }
    
    # Get appropriate disaster information
    info = disaster_info.get(disaster_type, disaster_info['general'])
    
    # Format the response
    response = f"I couldn't find specific guides in our database, but here's important safety information:\n\n**{info['title']}**\n{info['content']}\n\nFor the most current information, please check official local emergency management websites and follow guidance from authorities."
    
    return response


async def information_guide_node(state: AgentState) -> Dict[str, Any]: # LangGraphãƒãƒ¼ãƒ‰
    """
    æƒ…å ±ãƒ»ã‚¬ã‚¤ãƒ‰æä¾›ãƒãƒ¼ãƒ‰ã€‚
    - å†…éƒ¨é˜²ç½ã‚¬ã‚¤ãƒ‰ã®æä¾› (IG-001, IG-003)
    - Webæ¤œç´¢ã«ã‚ˆã‚‹æƒ…å ±è£œè¶³ (IG-002, IG-003)
    - éç½å®³é–¢é€£ã®ä¸€èˆ¬çš„ãªè³ªå•ã¸ã®é™å®šçš„å¯¾å¿œ (IG-004)
    
    ãƒãƒƒãƒå‡¦ç†ç‰ˆï¼š1å›ã®LLMå‘¼ã³å‡ºã—ã§å®Œå…¨ãªå¿œç­”ã‚’ç”Ÿæˆ
    """
    from langchain_core.messages import AIMessage

    user_input = state.get("user_input", "")
    user_language = state.get("user_language", "ja")
    current_task_type = state.get("current_task_type", "unknown_intent")
    is_disaster_mode = state.get("is_disaster_mode", False)
    
    # enhance_qualityã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—ãƒ»æ´»ç”¨
    improvement_feedback = state.get('improvement_feedback', '')
    if improvement_feedback:
        logger.info(f"ğŸ”„ Processing with improvement feedback: {improvement_feedback}")
    else:
        logger.info("ğŸ†• Initial processing (no improvement feedback)")
    
    # primary_intentã‹ã‚‰current_task_typeã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ä¿®æ­£
    primary_intent = state.get("primary_intent", "")
    if hasattr(primary_intent, 'value'):
        primary_intent = primary_intent.value
    elif isinstance(primary_intent, str) and primary_intent.startswith("IntentCategory."):
        primary_intent = primary_intent.replace("IntentCategory.", "").lower()
    
    # disaster_preparationã®å ´åˆã¯é©åˆ‡ãªã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’è¨­å®š
    if primary_intent in ["disaster_preparation", "disaster_information", "preparation_guide"] or current_task_type == "unknown_intent":
        if primary_intent == "disaster_preparation":
            current_task_type = "disaster_preparation"
        elif primary_intent == "preparation_guide":
            current_task_type = "disaster_preparation"
        elif primary_intent == "disaster_information":
            current_task_type = "guide_request"
        elif "æº–å‚™" in user_input or "å¯¾ç­–" in user_input or "å‚™ãˆ" in user_input or "preparation" in user_input.lower():
            current_task_type = "disaster_preparation"
        else:
            current_task_type = "guide_request"
    
    logger.info(f"Task type mapping: primary_intent='{primary_intent}' -> current_task_type='{current_task_type}'")
    
    # æ„Ÿæƒ…çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡ºã‚’ä¸¦åˆ—åŒ–ã®ãŸã‚å¾Œã§å®Ÿè¡Œ
    emotional_context_task = None
    if current_task_type not in ["general_question_non_disaster", "chitchat"]:
        # ç½å®³é–¢é€£ã®è³ªå•ã®å ´åˆã®ã¿æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œ
        from app.services.emotional_detector_llm import detect_emotional_state_llm
        emotional_context_task = asyncio.create_task(detect_emotional_state_llm(user_input, user_language))

    node_response_text_parts: List[str] = []
    node_generated_cards: List[Dict[str, Any]] = []

    # ãƒ„ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
    try:
        from app.tools.guide_tools import get_guide_search_tool
        guide_search_tool = get_guide_search_tool()
    except Exception as e:
        logger.warning(f"Failed to get guide search tool: {e}")
        guide_search_tool = None
    web_search_tool = get_web_search_tool()
    
    if not web_search_tool:
        logger.warning("Web search tool not available. Some functionality may be limited.")

    logger.info(f"Information guide node activated. Task: {current_task_type}, Disaster mode: {is_disaster_mode}, Batch processing: {USE_BATCH_PROCESSING}")
    
    # ãƒãƒƒãƒå‡¦ç†ç‰ˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    if USE_BATCH_PROCESSING:
        return await _information_guide_node_batch(state, current_task_type, user_input, user_language, is_disaster_mode)
    
    # å¾“æ¥ç‰ˆã®å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

    # --- IG-004: éç½å®³é–¢é€£ã®è©±é¡Œã¸ã®å¯¾å¿œ (å¹³å¸¸æ™‚ã®ã¿) ---
    if not is_disaster_mode and current_task_type in ["chitchat", "general_question_non_disaster"]:
        logger.info(f"Handling non-disaster topic (IG-004): type='{current_task_type}', query='{user_input}'")

        data_for_llm: Dict[str, Any] = {"original_query": user_input}

        if current_task_type == "general_question_non_disaster":
            try:
                # Check if test mode is enabled
                from app.config import app_settings
                if app_settings.test_mode and app_settings.environment != "production":
                    logger.info("Test mode: Web search disabled for non-disaster general questions")
                    data_for_llm["search_error"] = "Web search is disabled in test mode"
                elif not web_search_tool:
                    logger.warning("Web search tool not available for general question")
                    data_for_llm["search_error"] = "Web search not available"
                else:
                    # Webæ¤œç´¢ç”¨ã«æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’æº–å‚™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãç¿»è¨³ï¼‰
                    japanese_web_query = await _get_cached_japanese_query(user_input, "web_search")

                    # For non-disaster related questions, get web search with content summary
                    search_results_raw = await web_search_tool.ainvoke(input={
                        "query": japanese_web_query,
                        "num_results": 1, # 1 result is sufficient
                        "summarize_content": True, # Request content summary
                        "target_language": "ja"  # Process in Japanese
                })
                # SearchResultItemã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
                if search_results_raw:
                    # Pydanticãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                    data_for_llm["web_results"] = [item for item in search_results_raw]
                    logger.info(f"Web search for non-disaster query '{user_input}' successful with summarization.")
                else:
                    logger.info(f"No web search results for non-disaster query '{user_input}'.")
            except Exception as e:
                logger.error(f"Error during web search for non-disaster query '{user_input}': {e}", exc_info=True)
                data_for_llm["web_search_error"] = "An error occurred during web search."

        # Process with LLM in user's language for normal responses
        llm_processed_output = await _invoke_llm_for_task_specific_processing(
            task_prompt_template=INFORMATION_GUIDE_RESPONSE_PROMPT_TEMPLATE,
            user_language=user_language,  # Use app-specified language
            data_to_process=data_for_llm,
            user_input=user_input
        )
        if llm_processed_output.get("processed_text_for_user"):
            node_response_text_parts.append(llm_processed_output["processed_text_for_user"])
        if llm_processed_output.get("suggestion_card_data"):
            node_generated_cards.append(llm_processed_output["suggestion_card_data"])

    # --- IG-001, IG-003: å†…éƒ¨é˜²ç½ã‚¬ã‚¤ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æä¾› ---
    elif current_task_type in ["guide_contents_inquiry", "guide_request", "disaster_related", "disaster_guide_request", "disaster_preparation"]:
        # æ„å›³åˆ†é¡ã§æŠ½å‡ºã•ã‚ŒãŸã‚¬ã‚¤ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å…¨ä½“ã‚’ã‚¯ã‚¨ãƒªã«
        guide_query = state.get("intermediate_results", {}).get("extracted_entities", {}).get("guide_topic", user_input)
        logger.info(f"Handling guide content inquiry (IG-001): Query='{guide_query}'")

        try:
            # RAGæ¤œç´¢ç”¨ã«æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’æº–å‚™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãç¿»è¨³ï¼‰
            japanese_query = await _get_cached_japanese_query(guide_query, "rag_search")

            # GuideSearchToolã‚’æ—¥æœ¬èªã‚¯ã‚¨ãƒªã§å‘¼ã³å‡ºã—
            if guide_search_tool:
                guide_tool_results_raw = await guide_search_tool.search_guides(query=japanese_query, max_results=3) # é–¢é€£æ€§ã®é«˜ã„3ä»¶ã‚’å–å¾—
            else:
                logger.warning("Guide search tool not available, using empty results")
                guide_tool_results_raw = []

            if guide_tool_results_raw:
                # GuideContentã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
                # Pydanticãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
                data_for_llm = {"guide_content": [item for item in guide_tool_results_raw], "original_query": user_input}

                # ã‚¬ã‚¤ãƒ‰æ¤œç´¢çµæœã‚’ã‚«ãƒ¼ãƒ‰å½¢å¼ã§è¡¨ç¤º
                for idx, guide in enumerate(guide_tool_results_raw[:3]):  # æœ€å¤§3ä»¶
                    # ã‚¬ã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‰ã®ä½œæˆ
                    guide_card = {
                        "card_type": "guide_info",
                        "card_id": f"guide_{guide.get('id', idx)}",
                        "title": guide.get("title", ""),
                        "content": guide.get("content", guide.get("summary", ""))[:300] + "...",  # æœ€åˆã®300æ–‡å­—
                        "source": guide.get("source", "å†…é–£åºœé˜²ç½æƒ…å ±"),
                        "keywords": guide.get("keywords", []),
                        "action_query": f"{guide.get('title', '')}ã«ã¤ã„ã¦ã‚‚ã£ã¨è©³ã—ãæ•™ãˆã¦",
                        "priority": "medium"
                    }
                    node_generated_cards.append(guide_card)
                    logger.info(f"ğŸ“š Generated guide card {idx}: {guide_card['title']}")

                llm_processed_output = await _invoke_llm_for_task_specific_processing(
                    task_prompt_template=INFORMATION_GUIDE_RESPONSE_PROMPT_TEMPLATE,
                    user_language=user_language,  # Use app-specified language
                    data_to_process=data_for_llm,
                    user_input=user_input
                )
                if llm_processed_output.get("processed_text_for_user"):
                    node_response_text_parts.append(llm_processed_output["processed_text_for_user"])
                if llm_processed_output.get("suggestion_card_data"):
                    node_generated_cards.append(llm_processed_output["suggestion_card_data"])
            else:
                logger.warning(f"Guide for query '{guide_query}' not found or tool error. Trying fallback.")
                
                # Fallback handling
                from app.config import app_settings
                
                # In test mode, block web search but still try to generate context-aware fallback
                if app_settings.test_mode and app_settings.environment != "production" and web_search_tool and current_task_type in ["disaster_preparation", "guide_request"]:
                    logger.info("Test mode: Web search is disabled. Using context-aware fallback.")
                    # Extract disaster type and generate fallback
                    disaster_type = await _extract_disaster_type_from_query(user_input)
                    fallback_response = await _generate_context_aware_fallback(disaster_type, user_language)
                    node_response_text_parts.append(fallback_response)
                elif not app_settings.test_mode and web_search_tool and current_task_type in ["disaster_preparation", "guide_request"]:
                    try:
                        # Prepare Japanese query for web search
                        japanese_web_query = await _get_cached_japanese_query(user_input, "web_search")
                        
                        # Use LLM to enhance search query with relevant Japanese keywords
                        enhancement_prompt = f"""Enhance this Japanese search query for disaster preparation content.

Original query: "{japanese_web_query}"

Add relevant Japanese search keywords to find comprehensive preparation information.
Return ONLY the enhanced Japanese query, no explanations."""
                        
                        try:
                            enhanced_query = await ainvoke_llm(enhancement_prompt, task_type="translation", temperature=0.3, max_tokens=100)
                            japanese_web_query = enhanced_query.strip()
                        except Exception as e:
                            logger.warning(f"Query enhancement failed, using original: {e}")
                        
                        logger.info(f"Fallback web search with query: {japanese_web_query}")
                        
                        # Perform web search
                        search_results_raw = await web_search_tool.ainvoke(input={
                            "query": japanese_web_query,
                            "search_type": "preparation",
                            "max_results": 3,
                            "summarize_content": False
                        })
                        
                        if search_results_raw:
                            data_for_llm = {"web_results": [item for item in search_results_raw], "original_query": user_input}
                            
                            llm_processed_output = await _invoke_llm_for_task_specific_processing(
                                task_prompt_template=INFORMATION_GUIDE_RESPONSE_PROMPT_TEMPLATE,
                                user_language=user_language,
                                data_to_process=data_for_llm,
                                user_input=user_input
                            )
                            if llm_processed_output.get("processed_text_for_user"):
                                node_response_text_parts.append(llm_processed_output["processed_text_for_user"])
                            if llm_processed_output.get("suggestion_card_data"):
                                node_generated_cards.append(llm_processed_output["suggestion_card_data"])
                        else:
                            # No results from web search either
                            fallback_response = await _generate_context_aware_fallback("preparation", user_language)
                            node_response_text_parts.append(fallback_response)
                    except Exception as web_e:
                        logger.error(f"Web search fallback failed: {web_e}")
                        fallback_response = await _generate_context_aware_fallback("preparation", user_language)
                        node_response_text_parts.append(fallback_response)
                else:
                    # Extract disaster type from the query to provide context-aware fallback
                    disaster_type = await _extract_disaster_type_from_query(user_input)
                    
                    # Generate context-aware fallback response
                    fallback_response = await _generate_context_aware_fallback(disaster_type, user_language)
                    
                    node_response_text_parts.append(fallback_response)
        except Exception as e:
            logger.error(f"Error fetching or processing guide for '{guide_query}': {e}", exc_info=True)
            # Error in English (translation handled by response_generator)
            node_response_text_parts.append("An error occurred while retrieving guide information.")

    # --- IG-002, IG-003: Webæ¤œç´¢ã«ã‚ˆã‚‹æƒ…å ±è£œè¶³ (é˜²ç½é–¢é€£) ---
    elif current_task_type == "disaster_info_web_search":
        search_query = state.get("intermediate_results", {}).get("web_search_query", user_input)
        logger.info(f"Handling web search inquiry (IG-002): Query='{search_query}'")
        try:
            # Check if test mode is enabled
            from app.config import app_settings
            if app_settings.test_mode and app_settings.environment != "production":
                logger.info("Test mode: Web search is disabled for disaster info search")
                # Generate context-aware fallback instead
                disaster_type = await _extract_disaster_type_from_query(search_query)
                fallback_response = await _generate_context_aware_fallback(disaster_type, user_language)
                node_response_text_parts.append(fallback_response)
            elif not web_search_tool:
                logger.warning("Web search tool not available for disaster info search")
                # Error in English (translation handled by response_generator)
                node_response_text_parts.append("Web search service is not available. Please try again later.")
            else:
                # Webæ¤œç´¢ç”¨ã«æ—¥æœ¬èªã‚¯ã‚¨ãƒªã‚’æº–å‚™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãç¿»è¨³ï¼‰
                japanese_search_query = await _get_cached_japanese_query(search_query, "disaster_web_search")

                # Call web search tool, summary handled by LLM so summarize_content=False
                search_results_raw = await web_search_tool.ainvoke(input={
                    "query": japanese_search_query,
                    "num_results": 3, # Get multiple results
                    "summarize_content": False, # Summary handled by LLM
                    "target_language": "ja"  # Process in Japanese
                })
                
                if search_results_raw:
                    # SearchResultItemã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
                    data_for_llm = {"web_results": [item for item in search_results_raw], "original_query": user_input}

                    llm_processed_output = await _invoke_llm_for_task_specific_processing(
                        task_prompt_template=INFORMATION_GUIDE_RESPONSE_PROMPT_TEMPLATE,
                        user_language=user_language,  # Use app-specified language
                        data_to_process=data_for_llm,
                        user_input=user_input
                    )
                    if llm_processed_output.get("processed_text_for_user"):
                        node_response_text_parts.append(llm_processed_output["processed_text_for_user"])
                    if llm_processed_output.get("suggestion_card_data"):
                        node_generated_cards.append(llm_processed_output["suggestion_card_data"])
                else:
                    # Webæ¤œç´¢çµæœãŒç©ºã®å ´åˆã€LLMã‚’å‘¼ã³å‡ºã•ãšã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
                    logger.info(f"No web search results for query '{search_query}'. Using fallback message.")
                    # Fallback in English (translation handled by response_generator)
                    node_response_text_parts.append(f"No web information found for '{search_query}'.")
        except Exception as e:
            logger.error(f"Error during web search for '{search_query}': {e}", exc_info=True)
            # Error in English (translation handled by response_generator)
            node_response_text_parts.append("An error occurred during web search.")
    else:
        # ã©ã®å‡¦ç†ã«ã‚‚å½“ã¦ã¯ã¾ã‚‰ãªã‹ã£ãŸå ´åˆ (ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãŒä¸æ˜ã€ã¾ãŸã¯ã“ã®ãƒãƒ¼ãƒ‰ã®æ‹…å½“å¤–)
        if not node_response_text_parts:
            logger.warning(f"Information guide node reached end without specific action for task: {current_task_type}. User input: {user_input}")
            # Fallback in English (translation handled by response_generator)
            node_response_text_parts.append("I couldn't understand your question properly. Could you please ask in different words?")

    # æ„Ÿæƒ…çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—ã¨æ„Ÿæƒ…çš„ã‚µãƒãƒ¼ãƒˆå¿œç­”ã®ç”Ÿæˆ
    if emotional_context_task:
        try:
            emotional_context = await emotional_context_task
            state['emotional_context'] = emotional_context
            
            # æ„Ÿæƒ…çš„ã‚µãƒãƒ¼ãƒˆãŒå¿…è¦ãªå ´åˆã¯ãƒ•ãƒ©ã‚°ã‚’è¨­å®šï¼ˆãŸã ã—å…·ä½“çš„ãªæƒ…å ±è¦æ±‚ã®å ´åˆã¯æŠ‘åˆ¶ï¼‰
            # Check if this is a specific information request that should not prioritize emotional support
            is_specific_info_request = (
                current_task_type in ["disaster_preparation", "guide_contents_inquiry", "guide_request"] and
                emotional_context.get('intensity', 0) < 3  # Only override for low-medium emotional intensity
            )
            
            if emotional_context['should_prioritize'] and not is_specific_info_request:
                state['requires_emotional_support'] = True
                state['emotional_priority'] = 'high'
                # Information Guide - Emotional support priority enabled
            elif is_specific_info_request:
                logger.info(f"ğŸ“š Information Guide - Prioritizing information delivery over emotional support for {current_task_type}")
                
                # æ„Ÿæƒ…çš„ã‚µãƒãƒ¼ãƒˆå¿œç­”ã‚’ç”Ÿæˆï¼ˆãŸã ã—ã€å…·ä½“çš„ãªæƒ…å ±è¦æ±‚ã®å ´åˆã¯æŠ‘åˆ¶ï¼‰
                # disaster_preparationã‚¿ã‚¹ã‚¯ã®å ´åˆã¯å…·ä½“çš„ãªæƒ…å ±ã‚’å„ªå…ˆ
                if emotional_context.get('emotional_state') != 'neutral' and current_task_type not in ["disaster_preparation", "guide_contents_inquiry"]:
                    logger.info(f"ğŸ«‚ Information Guide - Generating emotional support response")
                    
                    # ç½å®³é–¢é€£ã®å ´åˆã¯ "disaster" ã‚’ã€ãã†ã§ãªã‘ã‚Œã° "general" ã‚’æŒ‡å®š
                    query_type = "disaster" if current_task_type in ["disaster_related", "guide_request", "disaster_guide_request"] else "general"
                    
                    emotional_response = await _generate_emotional_support_response_for_guide(
                        emotional_context, user_language, query_type
                    )
                    
                    # æ„Ÿæƒ…çš„ã‚µãƒãƒ¼ãƒˆå¿œç­”ã‚’å„ªå…ˆã—ã€æ—¢å­˜ã®å¿œç­”ã¯å¾Œã‚ã«è¿½åŠ 
                    if emotional_response:
                        node_response_text_parts.insert(0, emotional_response)
                        # Information Guide - Emotional support response prepended
        except Exception as e:
            logger.error(f"Failed to get emotional context: {e}")
            # Continue without emotional support

    final_response_main_text = "\n".join(filter(None, node_response_text_parts))

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ (BaseMessageå‹ã§çµ±ä¸€)
    response_message = AIMessage(
        content=final_response_main_text,
        additional_kwargs={
            "cards": node_generated_cards,
            "task_type": current_task_type
        }
    )

    updated_intermediate_results = {
        **(state.get("intermediate_results") or {}),
        "information_guide_output_main_text_raw": final_response_main_text,
    }

    current_cards_queue = state.get("cards_to_display_queue", [])
    if not isinstance(current_cards_queue, list): current_cards_queue = []
    updated_cards_queue = current_cards_queue + node_generated_cards

    logger.info(f"Information guide node finished. Main text (brief): '{final_response_main_text[:50]}...', Cards to add: {len(node_generated_cards)}")

    # Ensure we return a dict with required fields
    updates = {
        "messages": [response_message],
        "intermediate_results": updated_intermediate_results,
        "cards_to_display_queue": updated_cards_queue,
        "current_task_type": ["task_complete_information_guide"],
        "secondary_intents": []
    }
    return {
        **updates,
        "messages": updates.get("messages", []),
        "chat_history": state.messages if hasattr(state, 'messages') else [],
        "last_response": final_response_main_text,
        "final_response_text": final_response_main_text,  # è¿½åŠ : final_response_textãŒæ¬ è½ã—ã¦ã„ãŸ
        "intermediate_results": {
            **getattr(state, 'intermediate_results', {}),
            **updates.get("intermediate_results", {})
        }
    }


async def _generate_emotional_support_response_for_guide(
    emotional_context: Dict[str, Any], 
    user_language: str, 
    query_type: str
) -> str:
    """
    Generate emotional support response using LLM for information guide handler
    """
    emotional_state = emotional_context.get('emotional_state', 'anxious')
    intensity = emotional_context.get('intensity', 1)
    support_level = emotional_context.get('support_level', 'moderate')
    
    prompt = f"""You are LinguaSafeTrip, a compassionate disaster prevention assistant.
    
User's emotional state: {emotional_state} (intensity: {intensity}/3)
Support level needed: {support_level}
Query type: {query_type}
Target language: {user_language}

Generate a warm, empathetic response that:
1. Acknowledges their emotional state
2. Provides reassurance and support
3. Offers practical steps they can take
4. Makes them feel heard and supported

The response should be natural and conversational, not formulaic.
Focus on emotional support while being helpful with their {query_type} query.

Generate the response in English (it will be translated by response_generator)."""
    
    try:
        response = await ainvoke_llm(prompt, task_type="emotional_support", temperature=0.7)
        return response.strip()
    except Exception as e:
        logger.error(f"Failed to generate emotional support response: {e}")
        # Fallback to the template-based approach
        return generate_emotional_support_response(emotional_context, user_language, query_type)


async def _information_guide_node_batch(
    state: AgentState, 
    current_task_type: str, 
    user_input: str, 
    user_language: str, 
    is_disaster_mode: bool
) -> Dict[str, Any]:
    """
    ãƒãƒƒãƒå‡¦ç†ç‰ˆã®æƒ…å ±ã‚¬ã‚¤ãƒ‰ãƒãƒ¼ãƒ‰
    ã‚¬ã‚¤ãƒ‰æ¤œç´¢ã€Webæ¤œç´¢ã€å¿œç­”ç”Ÿæˆã€ã‚«ãƒ¼ãƒ‰ç”Ÿæˆã€å“è³ªãƒã‚§ãƒƒã‚¯ã‚’1å›ã®LLMå‘¼ã³å‡ºã—ã§å‡¦ç†
    """
    try:
        intent = state.get("primary_intent", "information_guide")
        
        # 1. ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
        search_tasks = []
        guide_results = []
        web_results = []
        
        # ã‚¬ã‚¤ãƒ‰æ¤œç´¢
        try:
            from app.tools.guide_tools import get_guide_search_tool
            guide_tool = get_guide_search_tool()
            if guide_tool:
                japanese_query = await _get_cached_japanese_query(user_input, "guide_search")
                search_tasks.append(("guide", guide_tool.search_guides(japanese_query, max_results=3)))
        except Exception as e:
            logger.warning(f"Guide search setup failed: {e}")
        
        # Webæ¤œç´¢
        try:
            web_tool = get_web_search_tool()
            if web_tool:
                web_japanese_query = await _get_cached_japanese_query(user_input, "web_search")
                search_tasks.append(("web", web_tool.ainvoke({
                    "query": web_japanese_query,
                    "num_results": 3,
                    "summarize_content": True,
                    "target_language": "ja"
                })))
        except Exception as e:
            logger.warning(f"Web search setup failed: {e}")
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        if search_tasks:
            results = await asyncio.gather(*[task[1] for task in search_tasks], return_exceptions=True)
            for i, (task_type, result) in enumerate(zip([task[0] for task in search_tasks], results)):
                if isinstance(result, Exception):
                    logger.warning(f"{task_type} search failed: {result}")
                else:
                    if task_type == "guide":
                        guide_results = result if result else []
                    elif task_type == "web":
                        web_results = result if result else []
        
        # 2. å®Œå…¨å¿œç­”ç”Ÿæˆï¼ˆ1å›ã®LLMå‘¼ã³å‡ºã—ï¼‰
        response_data = await CompleteResponseGenerator.generate_complete_response(
            user_input=user_input,
            intent=intent,
            user_language=user_language,
            context_data={
                "emotional_context": state.get("emotional_context", {}),
                "location_info": state.get("location_info", {}),
                "is_emergency_mode": is_disaster_mode,
                "task_type": current_task_type
            },
            handler_type="guide",
            search_results=web_results,
            guide_content=guide_results
        )
        
        # 3. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        from langchain_core.messages import AIMessage
        message = AIMessage(
            content=response_data["main_response"],
            additional_kwargs={
                "cards": response_data["suggestion_cards"],
                "follow_up_questions": response_data["follow_up_questions"],
                "priority": response_data["priority_level"],
                "handler_type": "guide"
            }
        )
        
        # 4. çµæœã‚’è¿”ã™
        result = {
            "messages": [message],
            "final_response_text": response_data["main_response"],
            "quality_self_check": response_data["quality_self_check"],
            "handler_completed": True,
            "last_response": response_data["main_response"],
            "chat_history": state.get("messages", []),
            "intermediate_results": {
                **state.get("intermediate_results", {}),
                "batch_processing_used": True,
                "guide_search_results": len(guide_results),
                "web_search_results": len(web_results)
            },
            "cards_to_display_queue": response_data["suggestion_cards"]  # ã‚«ãƒ¼ãƒ‰ã‚’ã¡ã‚ƒã‚“ã¨è¿”ã™
        }
        
        # Batch guide processing completed
        return result
        
    except Exception as e:
        logger.error(f"Batch guide processing failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒƒãƒå‡¦ç†ã‚’ç„¡åŠ¹ã«ã—ã¦å¾“æ¥å‡¦ç†ã‚’å®Ÿè¡Œ
        logger.info("Falling back to traditional processing")
        global USE_BATCH_PROCESSING
        original_batch_setting = USE_BATCH_PROCESSING
        USE_BATCH_PROCESSING = False
        try:
            return await information_guide_node(state)
        finally:
            USE_BATCH_PROCESSING = original_batch_setting
