"""
Webæ¤œç´¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
å®Ÿéš›ã®Webæ¤œç´¢çµæœã‚’ä¿å­˜ã—ã€ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ´»ç”¨
"""

import json
import logging
import os
import asyncio
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from pathlib import Path

from app.tools.web_search_tools import get_web_search_tool
from app.config import app_settings

logger = logging.getLogger(__name__)

@dataclass
class CachedNewsItem:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®"""
    title: str
    content: str
    url: str
    source: str
    published_at: str
    collected_at: str
    search_query: str
    relevance_score: float = 0.0
    category: str = "general"

class WebNewsCacheManager:
    """Webæ¤œç´¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
    
    def __init__(self):
        self.cache_dir = Path(__file__).parent.parent / "resources" / "mock_data" / "web_news_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.normal_cache_file = self.cache_dir / "normal_web_news.json"
        self.emergency_cache_file = self.cache_dir / "emergency_web_news.json"
        
        self.web_search_tool = get_web_search_tool()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.max_cache_items = 50
        self.cache_expiry_days = 7
        
        logger.info(f"WebNewsCacheManager initialized - Cache dir: {self.cache_dir}")
    
    async def collect_and_cache_normal_news(self, force_refresh: bool = False) -> List[CachedNewsItem]:
        """å¹³å¸¸æ™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        try:
            # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if not force_refresh:
                cached_news = self._load_cached_news("normal")
                if cached_news and len(cached_news) >= 10:
                    logger.info(f"Using cached normal news: {len(cached_news)} items")
                    return cached_news
            
            # å¹³å¸¸æ™‚ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
            normal_queries = [
                "é˜²ç½å¯¾ç­– å‚™è“„ å®¶åº­",
                "åœ°éœ‡å¯¾ç­– å®¶å…·å›ºå®š å®‰å…¨",
                "å°é¢¨å¯¾ç­– é¿é›£ æº–å‚™",
                "é˜²ç½ã‚°ãƒƒã‚º ãŠã™ã™ã‚ å¿…éœ€å“",
                "ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ— ç¢ºèªæ–¹æ³•",
                "é¿é›£è¨“ç·´ åœ°åŸŸ å‚åŠ ",
                "éå¸¸é£Ÿ å‚™è“„ ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒã‚¯",
                "é˜²ç½ã‚¢ãƒ—ãƒª ç½å®³æƒ…å ±"
            ]
            
            collected_news = []
            
            for query in normal_queries:
                try:
                    # Webæ¤œç´¢å®Ÿè¡Œ
                    search_result = await self.web_search_tool.ainvoke({
                        "query": query,
                        "max_results": 5
                    })
                    
                    # æ¤œç´¢çµæœã¯ãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã•ã‚Œã‚‹
                    if search_result and isinstance(search_result, list):
                        for item in search_result:
                            news_item = CachedNewsItem(
                                title=item.get("title", ""),
                                content=item.get("snippet", ""),
                                url=str(item.get("link", "")),  # HttpUrlã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                                source=item.get("source_domain", ""),
                                published_at=datetime.now(timezone.utc).isoformat(),
                                collected_at=datetime.now(timezone.utc).isoformat(),
                                search_query=query,
                                relevance_score=item.get("relevance_score", 0.8),
                                category="prevention"
                            )
                            collected_news.append(news_item)
                    
                    # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Error searching for '{query}': {e}")
                    continue
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._save_cached_news(collected_news, "normal")
            
            logger.info(f"ğŸ“° Collected and cached {len(collected_news)} normal news items")
            return collected_news
            
        except Exception as e:
            logger.error(f"Error collecting normal news: {e}")
            return []
    
    async def collect_and_cache_emergency_news(self, force_refresh: bool = False) -> List[CachedNewsItem]:
        """ç·Šæ€¥æ™‚ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        try:
            # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if not force_refresh:
                cached_news = self._load_cached_news("emergency")
                if cached_news and len(cached_news) >= 10:
                    logger.info(f"Using cached emergency news: {len(cached_news)} items")
                    return cached_news
            
            logger.warning("ğŸš¨ Collecting fresh emergency disaster news from web...")
            
            # ç·Šæ€¥æ™‚ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
            emergency_queries = [
                "åœ°éœ‡é€Ÿå ± æœ€æ–° è¢«å®³çŠ¶æ³",
                "åœ°éœ‡ ä½™éœ‡ éœ‡åº¦ æƒ…å ±",
                "åœ°éœ‡ å¾©æ—§ ãƒ©ã‚¤ãƒ•ãƒ©ã‚¤ãƒ³ çŠ¶æ³",
                "æ´¥æ³¢è­¦å ± é¿é›£æŒ‡ç¤º",
                "å°é¢¨ æ¥è¿‘ äº¤é€šæƒ…å ±",
                "è±ªé›¨ æ²³å·æ°¾æ¿« é¿é›£",
                "åœé›» å¾©æ—§æƒ…å ± ãƒ©ã‚¤ãƒ•ãƒ©ã‚¤ãƒ³",
                "é¿é›£æ‰€ é–‹è¨­ æƒ…å ±",
                "ç·Šæ€¥äº‹æ…‹å®£è¨€ ç½å®³",
                "å®‰å¦ç¢ºèª ç½å®³ç”¨ä¼è¨€ãƒ€ã‚¤ãƒ¤ãƒ«"
            ]
            
            collected_news = []
            
            for query in emergency_queries:
                try:
                    # Webæ¤œç´¢å®Ÿè¡Œ
                    search_result = await self.web_search_tool.ainvoke({
                        "query": query,
                        "max_results": 5
                    })
                    
                    # æ¤œç´¢çµæœã¯ãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã•ã‚Œã‚‹
                    if search_result and isinstance(search_result, list):
                        for item in search_result:
                            news_item = CachedNewsItem(
                                title=item.get("title", ""),
                                content=item.get("snippet", ""),
                                url=str(item.get("link", "")),  # HttpUrlã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                                source=item.get("source_domain", ""),
                                published_at=datetime.now(timezone.utc).isoformat(),
                                collected_at=datetime.now(timezone.utc).isoformat(),
                                search_query=query,
                                relevance_score=item.get("relevance_score", 0.9),
                                category="emergency"
                            )
                            collected_news.append(news_item)
                    
                    # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Error searching for '{query}': {e}")
                    continue
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._save_cached_news(collected_news, "emergency")
            
            logger.warning(f"ğŸš¨ Collected and cached {len(collected_news)} emergency news items")
            return collected_news
            
        except Exception as e:
            logger.error(f"Error collecting emergency news: {e}")
            return []
    
    def _load_cached_news(self, news_type: str) -> List[CachedNewsItem]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            cache_file = self.normal_cache_file if news_type == "normal" else self.emergency_cache_file
            
            if not cache_file.exists():
                return []
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯
            now = datetime.now(timezone.utc)
            valid_items = []
            
            for item_data in data.get("items", []):
                collected_at = datetime.fromisoformat(item_data["collected_at"].replace('Z', '+00:00'))
                if (now - collected_at).days < self.cache_expiry_days:
                    valid_items.append(CachedNewsItem(**item_data))
            
            logger.info(f"Loaded {len(valid_items)} valid cached {news_type} news items")
            return valid_items
            
        except Exception as e:
            logger.error(f"Error loading cached {news_type} news: {e}")
            return []
    
    def _save_cached_news(self, news_items: List[CachedNewsItem], news_type: str):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            cache_file = self.normal_cache_file if news_type == "normal" else self.emergency_cache_file
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
            existing_items = self._load_cached_news(news_type)
            
            # é‡è¤‡é™¤å»ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
            existing_urls = {item.url for item in existing_items}
            new_items = [item for item in news_items if item.url not in existing_urls]
            
            # çµ±åˆãƒªã‚¹ãƒˆ
            all_items = existing_items + new_items
            
            # æœ€å¤§æ•°ã«åˆ¶é™
            if len(all_items) > self.max_cache_items:
                # æ–°ã—ã„ã‚‚ã®ã‹ã‚‰ä¿æŒ
                all_items = sorted(all_items, key=lambda x: x.collected_at, reverse=True)[:self.max_cache_items]
            
            # ä¿å­˜
            cache_data = {
                "cached_at": datetime.now(timezone.utc).isoformat(),
                "news_type": news_type,
                "total_items": len(all_items),
                "items": [asdict(item) for item in all_items]
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Saved {len(all_items)} {news_type} news items to cache")
            
        except Exception as e:
            logger.error(f"Error saving {news_type} news cache: {e}")
    
    def _extract_domain(self, url: str) -> str:
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡º"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc
            # www.ã‚’é™¤å»
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return "unknown"
    
    def get_random_cached_news(self, news_type: str, count: int = 5) -> List[CachedNewsItem]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        import random
        
        cached_items = self._load_cached_news(news_type)
        if not cached_items:
            return []
        
        # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
        selected_count = min(count, len(cached_items))
        selected_items = random.sample(cached_items, selected_count)
        
        logger.info(f"Selected {len(selected_items)} random {news_type} news items from cache")
        return selected_items
    
    def clear_cache(self, news_type: Optional[str] = None):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            if news_type is None or news_type == "normal":
                if self.normal_cache_file.exists():
                    self.normal_cache_file.unlink()
                    logger.info("Cleared normal news cache")
            
            if news_type is None or news_type == "emergency":
                if self.emergency_cache_file.exists():
                    self.emergency_cache_file.unlink()
                    logger.warning("Cleared emergency news cache")
                    
        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
    
    async def refresh_all_caches(self):
        """å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°"""
        logger.info("ğŸ”„ Refreshing all news caches...")
        
        # ä¸¦è¡Œã—ã¦å®Ÿè¡Œ
        tasks = [
            self.collect_and_cache_normal_news(force_refresh=True),
            self.collect_and_cache_emergency_news(force_refresh=True)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
web_news_cache_manager = WebNewsCacheManager()